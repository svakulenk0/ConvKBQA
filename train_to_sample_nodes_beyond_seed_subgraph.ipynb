{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6720 conversations loaded\n",
      "7337 unique predicate labels in the KG\n"
     ]
    }
   ],
   "source": [
    "# load graph\n",
    "import json\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "\n",
    "from hdt import HDTDocument, TripleComponentRole\n",
    "\n",
    "from settings import *\n",
    "from predicates import properties\n",
    "\n",
    "\n",
    "hdt_file = 'wikidata2018_09_11.hdt'\n",
    "kg = HDTDocument(hdt_path+hdt_file)\n",
    "namespace = 'predef-wikidata2018-09-all'\n",
    "PREFIX_E = 'http://www.wikidata.org/entity/'\n",
    "PREFIX_P = 'http://www.wikidata.org/prop/'\n",
    "\n",
    "# prepare to retrieve all adjacent nodes including literals\n",
    "predicates_ids = []\n",
    "kg.configure_hops(1, predicates_ids, namespace, True, False)\n",
    "\n",
    "# load all predicate labels\n",
    "\n",
    "relationid2label = {}\n",
    "for p in properties['results']['bindings']:\n",
    "    _id = p['property']['value'].split('/')[-1]\n",
    "    label = p['propertyLabel']['value']\n",
    "    relationid2label[_id] = label\n",
    "\n",
    "# load the training dataset\n",
    "train_conversations_path = './data/train_set/train_set_ALL.json'\n",
    "\n",
    "with open(train_conversations_path, \"r\") as data:\n",
    "        conversations = json.load(data)\n",
    "print(\"%d conversations loaded\"%len(conversations))\n",
    "\n",
    "# model init\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "\n",
    "from MPBert_sampler_model import MessagePassingHDTBert\n",
    "from utils import adj\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# load all predicate labels\n",
    "from predicates import properties\n",
    "\n",
    "relationid2label = {}\n",
    "for p in properties['results']['bindings']:\n",
    "    _id = p['property']['value'].split('/')[-1]\n",
    "#     print(_id)\n",
    "    label = p['propertyLabel']['value']\n",
    "    relationid2label[_id] = label\n",
    "    \n",
    "# all unique predicate labels\n",
    "all_predicate_labels = list(relationid2label.values())\n",
    "all_predicate_ids = [kg.string_to_global_id(PREFIX_P + p, TripleComponentRole.PREDICATE) for p in list(relationid2label.keys())]\n",
    "assert len(all_predicate_labels) == len(all_predicate_ids)\n",
    "# print(all_predicate_ids[0])\n",
    "\n",
    "# model configuration\n",
    "model_name = 'bert-base-uncased'\n",
    "num_labels = 1\n",
    "num_entities = 12605  # size of the output layer, i.e., maximum number of entities in the subgraph that are candidate answers\n",
    "num_relations = 7337  # total number of all possible relations in the Wikidata KG TODO is it too big??\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "config = BertConfig.from_pretrained(model_name, num_labels=num_labels)\n",
    "\n",
    "model = MessagePassingHDTBert(config, num_entities, num_relations, mp_layer=True)\n",
    "# run model on the GPU\n",
    "# model.cuda()\n",
    "\n",
    "print(\"%d unique predicate labels in the KG\"%num_relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiled dataset with 4 samples\n"
     ]
    }
   ],
   "source": [
    "# check how many times an answer to the question fall into the initial (seed) subgraph separately for each order in the question sequence\n",
    "\n",
    "max_triples = 50000000\n",
    "offset = 0\n",
    "\n",
    "# collect only samples where the answer is entity and it is adjacent to the seed entity\n",
    "train_dataset = []\n",
    "\n",
    "graph_sizes = []\n",
    "max_n_edges = 2409 # max size of the graph allowed in the number of edges\n",
    "\n",
    "\n",
    "rdfsLabelURI='http://www.w3.org/2000/01/rdf-schema#label'\n",
    "\n",
    "def lookup_entity_labels(entity_ids):\n",
    "    # prepare mapping tensors with entity labels and ids\n",
    "    entity_labels, s_entity_ids = [], []\n",
    "    for i, e_id in enumerate(entity_ids):\n",
    "        e_uri = kg.global_id_to_string(e_id, TripleComponentRole.OBJECT)\n",
    "        (triples, cardinality) = kg.search_triples(e_uri, rdfsLabelURI, \"\")\n",
    "        if cardinality > 0:\n",
    "            label = triples.next()[2]\n",
    "            # strip language marker\n",
    "            label = label.split('\"')[1]\n",
    "            entity_labels.append(label)\n",
    "            s_entity_ids.append(e_id)\n",
    "\n",
    "    assert len(entity_labels) == len(s_entity_ids)\n",
    "    return entity_labels, s_entity_ids\n",
    "\n",
    "# consider a sample of the dataset\n",
    "n_limit = None\n",
    "if n_limit:\n",
    "    conversations = conversations[:n_limit]\n",
    "\n",
    "n_entities = []\n",
    "n_edges = []\n",
    "\n",
    "train_dataset = []\n",
    "\n",
    "for conversation in conversations[:100]:\n",
    "    # consider 1st questions only\n",
    "    for i in range(len(conversation['questions']))[:1]:\n",
    "        \n",
    "        question = conversation['questions'][i]['question']\n",
    "        answer = conversation['questions'][i]['answer']\n",
    "        # use oracle for the correct initial entity\n",
    "        seed_entity = conversation['seed_entity'].split('/')[-1]\n",
    "        seed_entity_id = kg.string_to_global_id(PREFIX_E+seed_entity, TripleComponentRole.OBJECT)\n",
    "\n",
    "        # retrieve all adjacent nodes including literals\n",
    "        subgraph = kg.compute_hops([seed_entity_id], max_triples, offset)\n",
    "        entity_ids, predicate_ids, adjacencies = subgraph\n",
    "        assert len(predicate_ids) == len(adjacencies)\n",
    "    #         print(\"conversation\")\n",
    "        \n",
    "        # consider only answers which are entities\n",
    "        if ('www.wikidata.org' in answer):\n",
    "            answer_id = kg.string_to_global_id(PREFIX_E+answer.split('/')[-1], TripleComponentRole.OBJECT)\n",
    "            in_subgraph = answer_id in entity_ids\n",
    "        \n",
    "            # retain samples with answer outside the seed subgraph\n",
    "            if not in_subgraph:\n",
    "                \n",
    "                p_input_ids = []\n",
    "                p_token_type_ids = []\n",
    "                p_attention_masks = []\n",
    "                \n",
    "                # prepare input of questions concatenated with all relation labels in the KG as candidates\n",
    "                for p_label in all_predicate_labels:\n",
    "\n",
    "                    # encode a text pair of the question with a predicate label\n",
    "                    encoded_dict = tokenizer.encode_plus(question, p_label,\n",
    "                                                         add_special_tokens=True,\n",
    "                                                         max_length=64,\n",
    "                                                         pad_to_max_length=True,\n",
    "                                                         return_attention_mask=True,\n",
    "                                                         return_token_type_ids=True)\n",
    "                    p_input_ids.append(encoded_dict['input_ids'])\n",
    "                    p_token_type_ids.append(encoded_dict['token_type_ids'])\n",
    "                    p_attention_masks.append(encoded_dict['attention_mask'])\n",
    "                    \n",
    "\n",
    "                # prepare input of questions concatenated with node labels as candidates: get labels for all candidate entities in the seed subgraph\n",
    "                entity_labels, entity_ids = lookup_entity_labels(entity_ids)\n",
    "                # create a batch of samples for each entity label separately\n",
    "                e_input_ids = []\n",
    "                e_token_type_ids = []\n",
    "                e_attention_masks = []\n",
    "                for e_label in entity_labels:\n",
    "                    # encode a text pair of the question with a predicate label\n",
    "                    encoded_dict = tokenizer.encode_plus(question, e_label,\n",
    "                                                         add_special_tokens=True,\n",
    "                                                         max_length=64,\n",
    "                                                         pad_to_max_length=True,\n",
    "                                                         return_attention_mask=True,\n",
    "                                                         return_token_type_ids=True)\n",
    "                    e_input_ids.append(encoded_dict['input_ids'])\n",
    "                    e_token_type_ids.append(encoded_dict['token_type_ids'])\n",
    "                    e_attention_masks.append(encoded_dict['attention_mask'])\n",
    "                assert len(e_input_ids) == len(entity_ids)\n",
    "                train_dataset.append([[torch.tensor(e_input_ids), torch.tensor(e_token_type_ids),\n",
    "                                       torch.tensor(e_attention_masks), torch.tensor(entity_ids)],\n",
    "                                      [torch.tensor(p_input_ids), torch.tensor(p_token_type_ids),\n",
    "                                       torch.tensor(p_attention_masks), torch.tensor(all_predicate_ids)],\n",
    "                                      torch.tensor([answer_id])])\n",
    "\n",
    "\n",
    "print(\"Compiled dataset with %d samples\" % len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training setup\n",
    "from transformers import get_linear_schedule_with_warmup, AdamW\n",
    "\n",
    "epochs = 4\n",
    "total_steps = len(train_dataset) * epochs\n",
    "\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8\n",
    "                 )\n",
    "# learning rate scheduler\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 training examples\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      "0\n",
      "tensor([5290])\n",
      "torch.Size([1, 18894])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-d10bee103ea9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;31m# backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Backprop\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/transformers/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/transformers/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "# train model (matching nodes and relations with a Transformer with subgraph sampling)\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# use CPU to train the model\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"%d training examples\"%(len(train_dataset)))\n",
    "# print(\"%d validation examples\"%(len(valid_dataset)))\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "    \n",
    "    # reset the total loss for this epoch\n",
    "    total_train_loss = 0\n",
    "    \n",
    "    # put the model into training mode\n",
    "    model.train()\n",
    "    \n",
    "    # for each sample of training data input as a batch of size 1\n",
    "    for step, batch in enumerate(train_dataset):\n",
    "        print(step)\n",
    "        e_inputs = [tensor.to(device) for tensor in batch[0]]\n",
    "        p_inputs = [tensor.to(device) for tensor in batch[1]]\n",
    "        labels = batch[2].to(device)\n",
    "        \n",
    "        model.zero_grad()\n",
    "        \n",
    "        # forward pass\n",
    "        loss, logits = model(e_inputs,\n",
    "                             p_inputs,\n",
    "                             labels)\n",
    "#         print(loss.item())\n",
    "        # accumulate the training loss over all of the batches\n",
    "        total_train_loss += loss.item()\n",
    "        \n",
    "        if not loss.item() == 0:\n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "            print(\"Backprop\")\n",
    "        \n",
    "        # clip gradient to prevent exploding\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    \n",
    "    # training epoch is over here\n",
    "    \n",
    "    # calculate average loss over all the batches\n",
    "    avg_train_loss = total_train_loss / len(train_dataset) \n",
    "    print(\"Average training loss: {0:.2f}\".format(avg_train_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
