{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Message-Passing Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 8, 9]\n",
      "(tensor([[0, 1],\n",
      "        [2, 3],\n",
      "        [8, 8],\n",
      "        [9, 1]]), (10, 5))\n"
     ]
    }
   ],
   "source": [
    "# test create a tensor with sparse matrices holding the adjacencies of the subgraph\n",
    "num_entities = 5  # size of the output layer, i.e., maximum number of entities in the subgraph that are candidate answers \n",
    "adjacencies = [[(0, 1), (2, 3)], [(3, 8), (4, 1)]]\n",
    "\n",
    "\n",
    "# adopted from https://github.com/pbloem/gated-rgcn/blob/a3dfa1cb162e2050c31f6e54bc21f0b6363bded2/kgmodels/util/util.py#L59\n",
    "def adj(adjacencies, num_nodes, vertical=True):\n",
    "    \"\"\"\n",
    "    Creates a sparse adjacency matrix for the given graph (the adjacency matrices of all\n",
    "    relations are stacked vertically by default) for a CPU.\n",
    "    :param adjacencies: list of lists of tuples representing the edges\n",
    "    :param num_nodes: maximum number of nodes in the graph\n",
    "    :return: sparse tensor\n",
    "    \"\"\"\n",
    "    ST = torch.sparse.FloatTensor\n",
    "\n",
    "    r, n = len(adjacencies), num_nodes\n",
    "    size = (r*n, n) if vertical else (n, r*n)\n",
    "\n",
    "    from_indices = []\n",
    "    upto_indices = []\n",
    "\n",
    "    for rel, edges in enumerate(adjacencies):\n",
    "        offset = rel * n\n",
    "        \n",
    "        for fr, to in edges:\n",
    "\n",
    "            if vertical:\n",
    "                fr = offset + fr\n",
    "            else:\n",
    "                to = offset + to\n",
    "\n",
    "            from_indices.append(fr)\n",
    "            upto_indices.append(to)\n",
    "    print(from_indices)\n",
    "    indices = torch.tensor([from_indices, upto_indices], dtype=torch.long)\n",
    "\n",
    "    return indices.t(), size\n",
    "\n",
    "\n",
    "A = adj(adjacencies, num_entities)\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input data pre-process adjacency matrix\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "\n",
    "def generate_adj_sp(edges, n_entities, include_inverse=True):\n",
    "    '''\n",
    "    Build adjacency matrix (sparse)\n",
    "    '''\n",
    "    adj_shape = (n_entities, n_entities)\n",
    "    # colect all predicate matrices separately into a list\n",
    "#     sp_adjacencies = []\n",
    "#     for edges in adjacencies:\n",
    "        # split subject (row) and object (col) node URIs\n",
    "    n_edges = len(edges)\n",
    "    row, col = np.transpose(edges)\n",
    "\n",
    "    # duplicate edges in the opposite direction\n",
    "    if include_inverse:\n",
    "        _row = np.hstack([row, col])\n",
    "        col = np.hstack([col, row])\n",
    "        row = _row\n",
    "        n_edges *= 2\n",
    "\n",
    "    # create adjacency matrix for this predicate\n",
    "    data = np.ones(n_edges)\n",
    "    adj = sp.csr_matrix((data, (row, col)), shape=adj_shape)#.todense()\n",
    "#     sp_adjacencies.append(adj)\n",
    "    return adj\n",
    "#     return np.asarray(sp_adjacencies)\n",
    "\n",
    "def to_torch_sparse_tensor(M):\n",
    "    M = M.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(np.vstack((M.row, M.col))).long()\n",
    "    values = torch.from_numpy(M.data)\n",
    "    shape = torch.Size(M.shape)\n",
    "    T = torch.sparse.FloatTensor(indices, values, shape)\n",
    "    return T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(indices=tensor([[0, 0, 0, 1, 1, 1],\n",
      "                       [0, 1, 2, 0, 1, 2]]),\n",
      "       values=tensor([-0.6693, -1.3099, -0.9233, -0.3582,  0.0946,  1.4149]),\n",
      "       size=(2, 3), nnz=6, layout=torch.sparse_coo, requires_grad=True)\n",
      "torch.Size([2, 3])\n",
      "-0.05607571452856064\n",
      "tensor(indices=tensor([[0, 0, 0, 1, 1, 1],\n",
      "                       [0, 1, 2, 0, 1, 2]]),\n",
      "       values=tensor([ 0.0375,  0.0735,  0.0518,  0.0201, -0.0053, -0.0793]),\n",
      "       size=(2, 3), nnz=6, layout=torch.sparse_coo, grad_fn=<MulBackward0>)\n",
      "torch.Size([2, 3])\n",
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [1.]], requires_grad=True)\n",
      "torch.Size([3, 1])\n",
      "tensor([[-2.9025],\n",
      "        [ 1.1513]], grad_fn=<SparseAddmmBackward>)\n",
      "torch.Size([2, 1])\n"
     ]
    }
   ],
   "source": [
    "# test sparse matrix multiplication functions used for MP \n",
    "import torch\n",
    "\n",
    "# random sparse matrix\n",
    "A = torch.randn(2, 3).to_sparse().requires_grad_(True)\n",
    "print(A)\n",
    "print(A.shape)\n",
    "\n",
    "# scalar multiplication\n",
    "b = torch.randn(1, 1, requires_grad=True).item()\n",
    "print(b)\n",
    "y = A * b\n",
    "print(y)\n",
    "print(y.shape)\n",
    "\n",
    "\n",
    "# dense vector multiplication\n",
    "b = torch.ones(3, 1, requires_grad=True)\n",
    "print(b)\n",
    "print(b.shape)\n",
    "y = torch.sparse.mm(A, b)\n",
    "print(y)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-f537414dbc8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_sparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0madjacencies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0magacencies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madjacencies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madjacencies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madjacencies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "# multiply several sparse matrices with a vector\n",
    "adjacencies = []\n",
    "for i in range(5):\n",
    "    # stack several separate adjacency matrices into a tensor by folding it into num_entities*num_relations rolls\n",
    "    A = torch.randn(2, 3).to_sparse().requires_grad_(True)\n",
    "    adjacencies.append(A)\n",
    "agacencies = torch.tensor(adjacencies)\n",
    "print(adjacencies)\n",
    "print(adjacencies.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model init\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "\n",
    "from MPBert_model import MessagePassingBert\n",
    "\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "# model configuration\n",
    "model_name = 'bert-base-uncased'\n",
    "num_labels = 1\n",
    "num_entities = 12605  # size of the output layer, i.e., maximum number of entities in the subgraph that are candidate answers \n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "config = BertConfig.from_pretrained(model_name, num_labels=num_labels)\n",
    "\n",
    "model = MessagePassingBert(config, num_entities)\n",
    "# run model on the GPU\n",
    "# model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(15.4030, grad_fn=<NllLossBackward>) tensor([[-0.2484],\n",
      "        [-5.9617],\n",
      "        [-0.2484],\n",
      "        ...,\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000]], grad_fn=<SparseAddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "# test inference with a sample input, where input is a question and a predicate label along with the list of edges for this predicate\n",
    "question1 = \"Hello, my dog is cute\"\n",
    "adjacencies = [(0, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1)]\n",
    "output = 1\n",
    "\n",
    "adjacencies = generate_adj_sp(adjacencies, num_entities)\n",
    "\n",
    "# build input tensors\n",
    "input_ids = torch.tensor(tokenizer.encode(question1)).unsqueeze(0)  # Batch size 1\n",
    "graph = to_torch_sparse_tensor(adjacencies)\n",
    "labels = torch.tensor([output]).unsqueeze(0)  # Batch size 1\n",
    "\n",
    "# run inference\n",
    "outputs = model(input_ids, graph, labels=labels)\n",
    "loss, logits = outputs[:2]\n",
    "print(loss, logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.08836555480957\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "model.train()\n",
    "outputs = model(input_ids, graph, labels=labels)\n",
    "loss = outputs[0]\n",
    "current_loss = loss.item()\n",
    "print(current_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6720 conversations loaded\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "import json\n",
    "conversations_path = './data/train_set/train_set_ALL.json'\n",
    "\n",
    "with open(conversations_path, \"r\") as data:\n",
    "    conversations = json.load(data)\n",
    "print(\"%d conversations loaded\"%len(conversations))\n",
    "\n",
    "# load graph\n",
    "from hdt import HDTDocument, TripleComponentRole\n",
    "from settings import *\n",
    "\n",
    "hdt_file = 'wikidata2018_09_11.hdt'\n",
    "kg = HDTDocument(hdt_path+hdt_file)\n",
    "namespace = 'predef-wikidata2018-09-all'\n",
    "PREFIX_E = 'http://www.wikidata.org/entity/'\n",
    "\n",
    "# prepare to retrieve all adjacent nodes including literals\n",
    "predicates_ids = []\n",
    "kg.configure_hops(1, predicates_ids, namespace, True, False)\n",
    "\n",
    "# load all predicate labels\n",
    "from predicates import properties\n",
    "\n",
    "relationid2label = {}\n",
    "for p in properties['results']['bindings']:\n",
    "    _id = p['property']['value'].split('/')[-1]\n",
    "    label = p['propertyLabel']['value']\n",
    "    relationid2label[_id] = label\n",
    "\n",
    "# print(relationid2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({True: 1})\n",
      "Compiled dataset with 40 samples\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter, defaultdict\n",
    "\n",
    "\n",
    "def lookup_predicate_labels(predicate_ids):\n",
    "    p_labels_map = defaultdict(list)\n",
    "    for p_id in predicate_ids:\n",
    "        p_uri = kg.global_id_to_string(p_id, TripleComponentRole.PREDICATE)\n",
    "        label = p_uri.split('/')[-1]\n",
    "        if label in relationid2label:\n",
    "            label = relationid2label[label]\n",
    "        else:\n",
    "            label = label.split('#')[-1]\n",
    "        p_labels_map[label].append(p_id)\n",
    "    return p_labels_map\n",
    "\n",
    "\n",
    "answers_in_subgraph = Counter()\n",
    "\n",
    "def check_answer_in_subgraph(conversation, subgraph):\n",
    "    answer1 = conversation['questions'][0]['answer']\n",
    "    # consider only answers which are entities\n",
    "    if ('www.wikidata.org' in answer1):\n",
    "        answer1_id = kg.string_to_global_id(PREFIX_E+answer1.split('/')[-1], TripleComponentRole.OBJECT)\n",
    "        in_subgraph = answer1_id in entity_ids\n",
    "        answers_in_subgraph.update([in_subgraph])\n",
    "        # consider only answer entities that are in the subgraph\n",
    "        if in_subgraph:\n",
    "            answer1_idx = entity_ids.index(answer1_id)\n",
    "            return answer1_idx\n",
    "\n",
    "\n",
    "max_triples = 50000000\n",
    "offset = 0\n",
    "\n",
    "# collect only samples where the answer is entity and it is adjacent to the seed entity\n",
    "# graphs = []\n",
    "# labels = []\n",
    "train_dataset = []\n",
    "\n",
    "graph_sizes = []\n",
    "max_n_edges = 2409 # max size of the graph allowed in the number of edges\n",
    "\n",
    "\n",
    "for conversation in conversations[:4]:\n",
    "    question1 = conversation['questions'][0]['question']\n",
    "    # use oracle for the correct initial entity\n",
    "    seed_entity = conversation['seed_entity'].split('/')[-1]\n",
    "    seed_entity_id = kg.string_to_global_id(PREFIX_E+seed_entity, TripleComponentRole.OBJECT)\n",
    "    \n",
    "    # retrieve all adjacent nodes including literals\n",
    "    subgraph1 = kg.compute_hops([seed_entity_id], max_triples, offset)\n",
    "    entity_ids, predicate_ids, adjacencies = subgraph1\n",
    "    assert len(predicate_ids) == len(adjacencies)\n",
    "\n",
    "    # check that the answer is in the subgraph\n",
    "    answer1_idx = check_answer_in_subgraph(conversation, entity_ids)\n",
    "    if answer1_idx:\n",
    "        # get labels for all candidate predicates\n",
    "        p_labels_map = lookup_predicate_labels(predicate_ids)\n",
    "        \n",
    "        # create a batch of samples for each predicate label separately\n",
    "        \n",
    "        input_ids = []\n",
    "        attention_masks = []\n",
    "        token_type_ids = []\n",
    "        \n",
    "        for p_label, p_ids in p_labels_map.items():\n",
    "            \n",
    "            # encode a text pair of the question with a predicate label\n",
    "            encoded_dict = tokenizer.encode_plus(question1, p_label,\n",
    "                                                 add_special_tokens=True,\n",
    "                                                 max_length=64,\n",
    "                                                 pad_to_max_length=True,\n",
    "                                                 return_attention_mask=True,\n",
    "                                                 return_token_type_ids=True)\n",
    "            input_ids.append(encoded_dict['input_ids'])\n",
    "            token_type_ids.append(encoded_dict['token_type_ids'])\n",
    "            attention_masks.append(encoded_dict['attention_mask'])\n",
    "            \n",
    "            # get adjacencies only for the predicates sharing the same label\n",
    "            selected_adjacencies = []\n",
    "            for p_id in p_ids:\n",
    "                p_id_idx = predicate_ids.index(p_id)\n",
    "                # add all edges together\n",
    "                for edge in adjacencies[p_id_idx]:\n",
    "                    if edge not in selected_adjacencies:\n",
    "                        selected_adjacencies.append(edge)\n",
    "            \n",
    "            A = generate_adj_sp(selected_adjacencies, num_entities, include_inverse=True)\n",
    "            \n",
    "            \n",
    "        train_dataset.append([torch.tensor(input_ids),\n",
    "                              torch.tensor(token_type_ids),\n",
    "                              torch.tensor(attention_masks),\n",
    "                              to_torch_sparse_tensor(A),\n",
    "                              torch.tensor([answer1_idx])])\n",
    "\n",
    "# print(max(graph_sizes))\n",
    "print(answers_in_subgraph)\n",
    "print(\"Compiled dataset with %d samples\"%len(input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training setup\n",
    "from transformers import get_linear_schedule_with_warmup, AdamW\n",
    "\n",
    "epochs = 4\n",
    "total_steps = len(train_dataset) * epochs\n",
    "\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8\n",
    "                 )\n",
    "# learning rate scheduler\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.4202, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4415, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4416, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4423, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4418, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4800, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4419, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4418, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4419, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4418, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4419, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4418, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4417, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4419, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4419, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4419, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4418, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4419, grad_fn=<NllLossBackward>)\n",
      "tensor(11.1034, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4419, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4419, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4418, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4418, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4418, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4050, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4418, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4418, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4418, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4418, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4420, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4419, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4417, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4415, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4417, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4699, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4418, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4419, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4418, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4418, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4414, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4429, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4415, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4412, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4418, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4412, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4418, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4418, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4419, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4419, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4421, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4418, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4417, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4419, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4418, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4418, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4419, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4418, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4404, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4417, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4420, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4420, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4418, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4418, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4800, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4418, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4418, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4418, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4418, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4418, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4418, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4418, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4417, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4417, grad_fn=<NllLossBackward>)\n",
      "tensor(9.6534, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4419, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4418, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4419, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4417, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4425, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4418, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4419, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4416, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4419, grad_fn=<NllLossBackward>)\n",
      "tensor(9.6048, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4418, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4418, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4418, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4418, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4419, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4418, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4430, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4418, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4419, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4419, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4419, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4418, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4424, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4417, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4418, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4419, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4418, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4419, grad_fn=<NllLossBackward>)\n",
      "tensor(9.6469, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4419, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4420, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4418, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4418, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4419, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4418, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4417, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4421, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4419, grad_fn=<NllLossBackward>)\n",
      "tensor(9.3961, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4419, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4419, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4418, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4419, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4902, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4416, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4730, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4423, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4418, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4462, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4418, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4418, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4418, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4418, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4418, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4418, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4420, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4420, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4418, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4418, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4419, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4420, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4415, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4417, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4419, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4419, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4419, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4418, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4325, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4419, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4419, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4419, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4419, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4431, grad_fn=<NllLossBackward>)\n",
      "tensor(9.5091, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4418, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4416, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4420, grad_fn=<NllLossBackward>)\n",
      "tensor(9.3222, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4418, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4419, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4418, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4417, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# set the seed value to make it reproducible\n",
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# use CPU to train the model\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "    # put the model into training mode\n",
    "    model.train()\n",
    "    \n",
    "    # for each sample of training data input as a batch of size 1\n",
    "    for step, batch in enumerate(train_dataset):\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_token_mask = batch[1].to(device)\n",
    "        b_input_mask = batch[2].to(device)\n",
    "        b_graphs = batch[3].to(device)\n",
    "        b_labels = batch[4].to(device)\n",
    "#         print(b_input_ids.shape)\n",
    "#         print(b_labels.shape)\n",
    "        model.zero_grad()\n",
    "        # forward pass\n",
    "        loss, logits = model(b_input_ids,\n",
    "                             b_graphs,\n",
    "                             token_type_ids=b_token_mask,\n",
    "                             attention_mask=b_input_mask,\n",
    "                             labels=b_labels)\n",
    "#         print(loss)\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        # TODO monitor training progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
