{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Message-Passing Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model init\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "\n",
    "from MPBert_model import MessagePassingBert\n",
    "\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "# model configuration\n",
    "model_name = 'bert-base-uncased'\n",
    "num_labels = 1\n",
    "# TODO pass as the model parameter to init\n",
    "num_entities = 12605  # size of the output layer, i.e., maximum number of entities in the subgraph that are candidate answers \n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "config = BertConfig.from_pretrained(model_name, num_labels=num_labels)\n",
    "model = MessagePassingBert(config, num_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.5585, grad_fn=<MseLossBackward>) tensor([[-0.2484]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "# test inference with a sample input, where input is a question and a predicate label along with the list of edges for this predicate\n",
    "question1 = \"Hello, my dog is cute\"\n",
    "adjacencies = [[(0, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1)]]\n",
    "output = 1\n",
    "\n",
    "# build input tensors\n",
    "input_ids = torch.tensor(tokenizer.encode(question1)).unsqueeze(0)  # Batch size 1\n",
    "graph = torch.tensor(adjacencies).unsqueeze(0)  # Batch size 1\n",
    "labels = torch.tensor([output]).unsqueeze(0)  # Batch size 1\n",
    "\n",
    "# run inference\n",
    "outputs = model(input_ids, graph, labels=labels)\n",
    "loss, logits = outputs[:2]\n",
    "print(loss, logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5900720357894897\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "model.train()\n",
    "outputs = model(input_ids, graph, labels=labels)\n",
    "loss = outputs[0]\n",
    "current_loss = loss.item()\n",
    "print(current_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6720 conversations loaded\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "import json\n",
    "conversations_path = './data/train_set/train_set_ALL.json'\n",
    "\n",
    "with open(conversations_path, \"r\") as data:\n",
    "    conversations = json.load(data)\n",
    "print(\"%d conversations loaded\"%len(conversations))\n",
    "\n",
    "# load graph\n",
    "from hdt import HDTDocument, TripleComponentRole\n",
    "from settings import *\n",
    "\n",
    "hdt_file = 'wikidata2018_09_11.hdt'\n",
    "kg = HDTDocument(hdt_path+hdt_file)\n",
    "namespace = 'predef-wikidata2018-09-all'\n",
    "PREFIX_E = 'http://www.wikidata.org/entity/'\n",
    "\n",
    "# prepare to retrieve all adjacent nodes including literals\n",
    "predicates_ids = []\n",
    "kg.configure_hops(1, predicates_ids, namespace, True, False)\n",
    "\n",
    "# load all predicate labels\n",
    "from predicates import properties\n",
    "\n",
    "relationid2label = {}\n",
    "for p in properties['results']['bindings']:\n",
    "    _id = p['property']['value'].split('/')[-1]\n",
    "    label = p['propertyLabel']['value']\n",
    "    relationid2label[_id] = label\n",
    "\n",
    "# print(relationid2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which author wrote the novel 1Q84?\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "tokenize() missing 1 required positional argument: 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-3ae6c18511eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     78\u001b[0m                 \u001b[0mselected_adjacencies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madjacencies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp_id_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m                 \u001b[0;31m# concatenate question with a predicate label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                 \u001b[0mquestion_predicate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencode_text_pair\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m                 \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mquestion_predicate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubgraph1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrect_answer_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-81-3ae6c18511eb>\u001b[0m in \u001b[0;36mencode_text_pair\u001b[0;34m(text_a, text_b, tokenizer)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mencode_text_pair\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_a\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mtokens_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_a\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0mtokens_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_b\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcls_token\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtokens_a\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msep_token\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtokens_b\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msep_token\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: tokenize() missing 1 required positional argument: 'text'"
     ]
    }
   ],
   "source": [
    "from collections import Counter, defaultdict\n",
    "\n",
    "\n",
    "def lookup_predicate_labels(predicate_ids):\n",
    "    p_labels_map = defaultdict(list)\n",
    "    for p_id in predicate_ids:\n",
    "        p_uri = kg.global_id_to_string(p_id, TripleComponentRole.PREDICATE)\n",
    "        label = p_uri.split('/')[-1]\n",
    "        if label in relationid2label:\n",
    "            label = relationid2label[label]\n",
    "        else:\n",
    "            label = label.split('#')[-1]\n",
    "        p_labels_map[label].append(p_id)\n",
    "    return p_labels_map\n",
    "\n",
    "\n",
    "answers_in_subgraph = Counter()\n",
    "\n",
    "def check_answer_in_subgraph(conversation, subgraph):\n",
    "    answer1 = conversation['questions'][0]['answer']\n",
    "    # consider only answers which are entities\n",
    "    if ('www.wikidata.org' in answer1):\n",
    "        answer1_id = kg.string_to_global_id(PREFIX_E+answer1.split('/')[-1], TripleComponentRole.OBJECT)\n",
    "        in_subgraph = answer1_id in entity_ids\n",
    "        answers_in_subgraph.update([in_subgraph])\n",
    "        # consider only answer entities that are in the subgraph\n",
    "        if in_subgraph:\n",
    "            answer1_idx = entity_ids.index(answer1_id)\n",
    "            return answer1_idx\n",
    "        \n",
    "\n",
    "# TODO tokenize and concatenate two text inputs with BERT tokenizer\n",
    "def encode_text_pair(text_a, text_b, tokenizer):\n",
    "    print(text_a)\n",
    "    tokens_a = tokenizer.tokenize(text_a)\n",
    "    tokens_b = tokenizer.tokenize(text_b)\n",
    "    tokens = [tokenizer.cls_token] + tokens_a + [tokenizer.sep_token] + tokens_b + [tokenizer.sep_token]\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    return input_ids\n",
    "\n",
    "\n",
    "max_triples = 50000000\n",
    "offset = 0\n",
    "\n",
    "# collect only samples where the answer is entity and it is adjacent to the seed entity\n",
    "dataset = []\n",
    "labels = []\n",
    "for conversation in conversations:\n",
    "    question1 = conversation['questions'][0]['question']\n",
    "    # use oracle for the correct initial entity\n",
    "    seed_entity = conversation['seed_entity'].split('/')[-1]\n",
    "    seed_entity_id = kg.string_to_global_id(PREFIX_E+seed_entity, TripleComponentRole.OBJECT)\n",
    "    \n",
    "    # retrieve all adjacent nodes including literals\n",
    "    subgraph1 = kg.compute_hops([seed_entity_id], max_triples, offset)\n",
    "    entity_ids, predicate_ids, adjacencies = subgraph1\n",
    "    assert len(predicate_ids) == len(adjacencies)\n",
    "\n",
    "    # check that the answer is in the subgraph\n",
    "    answer1_idx = check_answer_in_subgraph(conversation, entity_ids)\n",
    "    if answer1_idx:\n",
    "        # get labels for all candidate predicates\n",
    "        p_labels_map = lookup_predicate_labels(predicate_ids)\n",
    "        \n",
    "        # TODO pad the graph to the maximum size in the number of entities\n",
    "        \n",
    "        # create one-hot vector for the correct answer to the input question\n",
    "        correct_answer_vector = [0] * num_entities\n",
    "        correct_answer_vector[answer1_idx] = 1\n",
    "        \n",
    "        # create a sample for each predicate label separately\n",
    "        for p_label, p_ids in p_labels_map.items():\n",
    "            data = [p_label, p_ids]\n",
    "            # get adjacencies only for the predicates sharing the same label\n",
    "            selected_adjacencies = []\n",
    "            for p_id in p_ids:\n",
    "                p_id_idx = predicate_ids.index(p_id)\n",
    "                selected_adjacencies.append(adjacencies[p_id_idx])\n",
    "                # concatenate question with a predicate label\n",
    "                question_predicate = encode_text_pair(question1, p_label, BertTokenizer)\n",
    "                dataset.append([question_predicate, subgraph1])\n",
    "                labels.append(correct_answer_vector)\n",
    "\n",
    "\n",
    "print(answers_in_subgraph)\n",
    "print(\"Compiled dataset with %d samples\"%len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
