{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Message-Passing Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(indices=tensor([[0, 0, 0, 1, 1, 1],\n",
      "                       [0, 1, 2, 0, 1, 2]]),\n",
      "       values=tensor([-0.6693, -1.3099, -0.9233, -0.3582,  0.0946,  1.4149]),\n",
      "       size=(2, 3), nnz=6, layout=torch.sparse_coo, requires_grad=True)\n",
      "torch.Size([2, 3])\n",
      "-0.05607571452856064\n",
      "tensor(indices=tensor([[0, 0, 0, 1, 1, 1],\n",
      "                       [0, 1, 2, 0, 1, 2]]),\n",
      "       values=tensor([ 0.0375,  0.0735,  0.0518,  0.0201, -0.0053, -0.0793]),\n",
      "       size=(2, 3), nnz=6, layout=torch.sparse_coo, grad_fn=<MulBackward0>)\n",
      "torch.Size([2, 3])\n",
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [1.]], requires_grad=True)\n",
      "torch.Size([3, 1])\n",
      "tensor([[-2.9025],\n",
      "        [ 1.1513]], grad_fn=<SparseAddmmBackward>)\n",
      "torch.Size([2, 1])\n"
     ]
    }
   ],
   "source": [
    "# test sparse matrix multiplication functions used for MP \n",
    "import torch\n",
    "\n",
    "# random sparse matrix\n",
    "A = torch.randn(2, 3).to_sparse().requires_grad_(True)\n",
    "print(A)\n",
    "print(A.shape)\n",
    "\n",
    "# scalar multiplication\n",
    "b = torch.randn(1, 1, requires_grad=True).item()\n",
    "print(b)\n",
    "y = A * b\n",
    "print(y)\n",
    "print(y.shape)\n",
    "\n",
    "\n",
    "# dense vector multiplication\n",
    "b = torch.ones(3, 1, requires_grad=True)\n",
    "print(b)\n",
    "print(b.shape)\n",
    "y = torch.sparse.mm(A, b)\n",
    "print(y)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model init\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "\n",
    "from MPBert_model import MessagePassingBert\n",
    "\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "# model configuration\n",
    "model_name = 'bert-base-uncased'\n",
    "num_labels = 1\n",
    "num_entities = 12605  # size of the output layer, i.e., maximum number of entities in the subgraph that are candidate answers \n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "config = BertConfig.from_pretrained(model_name, num_labels=num_labels)\n",
    "\n",
    "model = MessagePassingBert(config, num_entities)\n",
    "# run model on the GPU\n",
    "# model.cuda()\n",
    "\n",
    "# input data pre-process adjacency matrix\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "\n",
    "def generate_adj_sp(edges, n_entities, include_inverse=True):\n",
    "    '''\n",
    "    Build adjacency matrix (sparse)\n",
    "    '''\n",
    "    adj_shape = (n_entities, n_entities)\n",
    "    # colect all predicate matrices separately into a list\n",
    "#     sp_adjacencies = []\n",
    "#     for edges in adjacencies:\n",
    "        # split subject (row) and object (col) node URIs\n",
    "    n_edges = len(edges)\n",
    "    row, col = np.transpose(edges)\n",
    "\n",
    "    # duplicate edges in the opposite direction\n",
    "    if include_inverse:\n",
    "        _row = np.hstack([row, col])\n",
    "        col = np.hstack([col, row])\n",
    "        row = _row\n",
    "        n_edges *= 2\n",
    "\n",
    "    # create adjacency matrix for this predicate\n",
    "    data = np.ones(n_edges)\n",
    "    adj = sp.csr_matrix((data, (row, col)), shape=adj_shape)#.todense()\n",
    "#     sp_adjacencies.append(adj)\n",
    "    return adj\n",
    "#     return np.asarray(sp_adjacencies)\n",
    "\n",
    "def to_torch_sparse_tensor(M):\n",
    "    M = M.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(np.vstack((M.row, M.col))).long()\n",
    "    values = torch.from_numpy(M.data)\n",
    "    shape = torch.Size(M.shape)\n",
    "    T = torch.sparse.FloatTensor(indices, values, shape)\n",
    "    return T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(15.4030, grad_fn=<NllLossBackward>) tensor([[-0.2484],\n",
      "        [-5.9617],\n",
      "        [-0.2484],\n",
      "        ...,\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000]], grad_fn=<SparseAddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "# test inference with a sample input, where input is a question and a predicate label along with the list of edges for this predicate\n",
    "question1 = \"Hello, my dog is cute\"\n",
    "adjacencies = [(0, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1)]\n",
    "output = 1\n",
    "\n",
    "adjacencies = generate_adj_sp(adjacencies, num_entities)\n",
    "\n",
    "# build input tensors\n",
    "input_ids = torch.tensor(tokenizer.encode(question1)).unsqueeze(0)  # Batch size 1\n",
    "graph = to_torch_sparse_tensor(adjacencies)\n",
    "labels = torch.tensor([output]).unsqueeze(0)  # Batch size 1\n",
    "\n",
    "# run inference\n",
    "outputs = model(input_ids, graph, labels=labels)\n",
    "loss, logits = outputs[:2]\n",
    "print(loss, logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.08836555480957\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "model.train()\n",
    "outputs = model(input_ids, graph, labels=labels)\n",
    "loss = outputs[0]\n",
    "current_loss = loss.item()\n",
    "print(current_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6720 conversations loaded\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "import json\n",
    "conversations_path = './data/train_set/train_set_ALL.json'\n",
    "\n",
    "with open(conversations_path, \"r\") as data:\n",
    "    conversations = json.load(data)\n",
    "print(\"%d conversations loaded\"%len(conversations))\n",
    "\n",
    "# load graph\n",
    "from hdt import HDTDocument, TripleComponentRole\n",
    "from settings import *\n",
    "\n",
    "hdt_file = 'wikidata2018_09_11.hdt'\n",
    "kg = HDTDocument(hdt_path+hdt_file)\n",
    "namespace = 'predef-wikidata2018-09-all'\n",
    "PREFIX_E = 'http://www.wikidata.org/entity/'\n",
    "\n",
    "# prepare to retrieve all adjacent nodes including literals\n",
    "predicates_ids = []\n",
    "kg.configure_hops(1, predicates_ids, namespace, True, False)\n",
    "\n",
    "# load all predicate labels\n",
    "from predicates import properties\n",
    "\n",
    "relationid2label = {}\n",
    "for p in properties['results']['bindings']:\n",
    "    _id = p['property']['value'].split('/')[-1]\n",
    "    label = p['propertyLabel']['value']\n",
    "    relationid2label[_id] = label\n",
    "\n",
    "# print(relationid2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({True: 1})\n",
      "Compiled dataset with 40 samples\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter, defaultdict\n",
    "\n",
    "\n",
    "def lookup_predicate_labels(predicate_ids):\n",
    "    p_labels_map = defaultdict(list)\n",
    "    for p_id in predicate_ids:\n",
    "        p_uri = kg.global_id_to_string(p_id, TripleComponentRole.PREDICATE)\n",
    "        label = p_uri.split('/')[-1]\n",
    "        if label in relationid2label:\n",
    "            label = relationid2label[label]\n",
    "        else:\n",
    "            label = label.split('#')[-1]\n",
    "        p_labels_map[label].append(p_id)\n",
    "    return p_labels_map\n",
    "\n",
    "\n",
    "answers_in_subgraph = Counter()\n",
    "\n",
    "def check_answer_in_subgraph(conversation, subgraph):\n",
    "    answer1 = conversation['questions'][0]['answer']\n",
    "    # consider only answers which are entities\n",
    "    if ('www.wikidata.org' in answer1):\n",
    "        answer1_id = kg.string_to_global_id(PREFIX_E+answer1.split('/')[-1], TripleComponentRole.OBJECT)\n",
    "        in_subgraph = answer1_id in entity_ids\n",
    "        answers_in_subgraph.update([in_subgraph])\n",
    "        # consider only answer entities that are in the subgraph\n",
    "        if in_subgraph:\n",
    "            answer1_idx = entity_ids.index(answer1_id)\n",
    "            return answer1_idx\n",
    "\n",
    "\n",
    "max_triples = 50000000\n",
    "offset = 0\n",
    "\n",
    "# collect only samples where the answer is entity and it is adjacent to the seed entity\n",
    "# input_ids = []\n",
    "# attention_masks = []\n",
    "# token_type_ids = []\n",
    "# graphs = []\n",
    "# labels = []\n",
    "train_dataset = []\n",
    "\n",
    "graph_sizes = []\n",
    "max_n_edges = 2409 # max size of the graph allowed in the number of edges\n",
    "\n",
    "\n",
    "for conversation in conversations[:4]:\n",
    "    question1 = conversation['questions'][0]['question']\n",
    "    # use oracle for the correct initial entity\n",
    "    seed_entity = conversation['seed_entity'].split('/')[-1]\n",
    "    seed_entity_id = kg.string_to_global_id(PREFIX_E+seed_entity, TripleComponentRole.OBJECT)\n",
    "    \n",
    "    # retrieve all adjacent nodes including literals\n",
    "    subgraph1 = kg.compute_hops([seed_entity_id], max_triples, offset)\n",
    "    entity_ids, predicate_ids, adjacencies = subgraph1\n",
    "    assert len(predicate_ids) == len(adjacencies)\n",
    "\n",
    "    # check that the answer is in the subgraph\n",
    "    answer1_idx = check_answer_in_subgraph(conversation, entity_ids)\n",
    "    if answer1_idx:\n",
    "        # get labels for all candidate predicates\n",
    "        p_labels_map = lookup_predicate_labels(predicate_ids)\n",
    "        \n",
    "        # create a sample for each predicate label separately\n",
    "        for p_label, p_ids in p_labels_map.items():\n",
    "            \n",
    "            # encode a text pair of the question with a predicate label\n",
    "            encoded_dict = tokenizer.encode_plus(question1, p_label,\n",
    "                                                 add_special_tokens=True,\n",
    "                                                 max_length=64,\n",
    "                                                 pad_to_max_length=True,\n",
    "                                                 return_attention_mask=True,\n",
    "                                                 return_token_type_ids=True)\n",
    "            input_ids.append(encoded_dict['input_ids'])\n",
    "            attention_masks.append(encoded_dict['attention_mask'])\n",
    "            token_type_ids.append(encoded_dict['token_type_ids'])\n",
    "            \n",
    "            # get adjacencies only for the predicates sharing the same label\n",
    "            selected_adjacencies = []\n",
    "            for p_id in p_ids:\n",
    "                p_id_idx = predicate_ids.index(p_id)\n",
    "#                 print(len(adjacencies[p_id_idx]))\n",
    "                # add edges for each predicate separately\n",
    "#                 selected_adjacencies.append(adjacencies[p_id_idx])\n",
    "                # add all edges together\n",
    "                for edge in adjacencies[p_id_idx]:\n",
    "                    if edge not in selected_adjacencies:\n",
    "#                         print(edge)\n",
    "                        selected_adjacencies.append(edge)\n",
    "            \n",
    "            A = generate_adj_sp(selected_adjacencies, num_entities, include_inverse=True)\n",
    "            # TODO normalise graph size: pad to the maximum number of entities\n",
    "            # num_entities\n",
    "#             n_edges = len(A)\n",
    "#             graph_sizes.append(n_edges)\n",
    "#             print(n_edges)\n",
    "#             print([len(a) for a in selected_adjacencies])\n",
    "#             selected_adjacencies[0]\n",
    "            # pad to max number of edges\n",
    "#             selected_adjacencies += [(0, 0)] * (max_n_edges - n_edges)\n",
    "#             print(selected_adjacencies)\n",
    "#             graphs.append(to_torch_sparse_tensor(A))\n",
    "#             labels.append(correct_answer_vector)\n",
    "            train_dataset.append([torch.tensor([encoded_dict['input_ids']]),\n",
    "                                  torch.tensor([encoded_dict['token_type_ids']]),\n",
    "                                  torch.tensor([encoded_dict['attention_mask']]),\n",
    "                                  to_torch_sparse_tensor(A),\n",
    "                                  torch.tensor([answer1_idx])])\n",
    "\n",
    "# print(max(graph_sizes))\n",
    "print(answers_in_subgraph)\n",
    "print(\"Compiled dataset with %d samples\"%len(input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training setup\n",
    "from transformers import get_linear_schedule_with_warmup, AdamW\n",
    "\n",
    "epochs = 4\n",
    "total_steps = len(train_dataset) * epochs\n",
    "\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8\n",
    "                 )\n",
    "# learning rate scheduler\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.4202, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4415, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4416, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4423, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4418, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4800, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4419, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4418, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4419, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4418, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4419, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4418, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4417, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4419, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4419, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4419, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4418, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4419, grad_fn=<NllLossBackward>)\n",
      "tensor(11.1034, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4419, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4419, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4418, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4418, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4418, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4050, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4418, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4418, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4418, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4418, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4420, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4419, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4417, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4415, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4417, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4699, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4418, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4419, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4418, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4418, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4414, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4429, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4415, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4412, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4418, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4412, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4418, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4418, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4419, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4419, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4421, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4418, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4417, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4419, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4418, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4418, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4419, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4418, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4404, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4417, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4420, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4420, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4418, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4418, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4800, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4418, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4418, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4418, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4418, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4418, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4418, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4418, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4417, grad_fn=<NllLossBackward>)\n",
      "tensor(9.4417, grad_fn=<NllLossBackward>)\n",
      "tensor(9.6534, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# set the seed value to make it reproducible\n",
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# use CPU to train the model\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "    # put the model into training mode\n",
    "    model.train()\n",
    "    \n",
    "    # for each sample of training data input as a batch of size 1\n",
    "    for step, batch in enumerate(train_dataset):\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_token_mask = batch[1].to(device)\n",
    "        b_input_mask = batch[2].to(device)\n",
    "        b_graphs = batch[3].to(device)\n",
    "        b_labels = batch[4].to(device)\n",
    "#         print(b_input_ids.shape)\n",
    "#         print(b_labels.shape)\n",
    "        model.zero_grad()\n",
    "        # forward pass\n",
    "        loss, logits = model(b_input_ids,\n",
    "                             b_graphs,\n",
    "                             token_type_ids=b_token_mask,\n",
    "                             attention_mask=b_input_mask,\n",
    "                             labels=b_labels)\n",
    "#         print(loss)\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        # TODO monitor training progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "transformers"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
