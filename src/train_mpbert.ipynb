{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load graph\n",
    "import json\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "\n",
    "from hdt import HDTDocument, TripleComponentRole\n",
    "\n",
    "from settings import *\n",
    "from predicates import properties\n",
    "\n",
    "\n",
    "hdt_file = 'wikidata2018_09_11.hdt'\n",
    "kg = HDTDocument(hdt_path+hdt_file)\n",
    "namespace = 'predef-wikidata2018-09-all'\n",
    "PREFIX_E = 'http://www.wikidata.org/entity/'\n",
    "PREFIX_P = 'http://www.wikidata.org/prop/'\n",
    "\n",
    "# prepare to retrieve all adjacent nodes including literals\n",
    "predicates_ids = []\n",
    "kg.configure_hops(1, predicates_ids, namespace, True, False)\n",
    "\n",
    "# load all predicate labels\n",
    "relationid2label = {}\n",
    "for p in properties['results']['bindings']:\n",
    "    _id = p['property']['value'].split('/')[-1]\n",
    "    label = p['propertyLabel']['value']\n",
    "    relationid2label[_id] = label\n",
    "\n",
    "# model init\n",
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertConfig\n",
    "\n",
    "from MPDistilBert_sampler_model import MessagePassingHDTBert\n",
    "from utils import adj\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# load all predicate labels\n",
    "from all_predicate_labels import all_predicate_labels\n",
    "\n",
    "print(\"Graph loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model init\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "from MPDistilBert_sampler_model import MessagePassingHDTBert\n",
    "\n",
    "DEVICE = 'cuda'\n",
    "\n",
    "# model configuration\n",
    "model_name = 'distilbert-base-uncased'\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "config = DistilBertConfig.from_pretrained(model_name, num_labels=1)\n",
    "\n",
    "# model configuration\n",
    "# model_name = 'bert-base-uncased'\n",
    "# tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "# regression task for matching predicate/entity label to the input question\n",
    "# config = BertConfig.from_pretrained(model_name, num_labels=1)\n",
    "# print(config.initializer_range)\n",
    "# config.initializer_range=0.8\n",
    "# print(config.initializer_range)\n",
    "\n",
    "E_BEAM = 10\n",
    "P_BEAM = 8000\n",
    "\n",
    "model = MessagePassingHDTBert(config, topk_entities=E_BEAM, topk_predicates=P_BEAM)\n",
    "\n",
    "for name, param in model.bert.named_parameters():                \n",
    "    if name.startswith('embeddings'):\n",
    "        param.requires_grad = False\n",
    "        \n",
    "# print(model.bert)\n",
    "\n",
    "# freeze only the first k-1 layers\n",
    "k = 6\n",
    "ct = 0\n",
    "for child in model.bert.transformer.layer.children():\n",
    "    ct += 1\n",
    "    if ct < k:\n",
    "        for param in child.parameters():\n",
    "            param.requires_grad = False\n",
    "    else:\n",
    "        print(\"Not frozen Transformer layer\")\n",
    "\n",
    "for name, param in model.bert.named_parameters():                \n",
    "    if param.requires_grad:\n",
    "        print(name)\n",
    "\n",
    "# for param in model.bert.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "\n",
    "if DEVICE == 'cuda':\n",
    "    device = torch.device(\"cuda\")\n",
    "    # run model on the GPU\n",
    "    model.cuda()\n",
    "else:\n",
    "    # use CPU to train the model\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Model loaded to\", DEVICE)\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how many times an answer to the question fall into the initial (seed) subgraph separately for each order in the question sequence\n",
    "NSAMPLES = 5\n",
    "max_triples = 50000\n",
    "offset = 0\n",
    "\n",
    "# dataset setup\n",
    "train_conversations_path = '../data/train_set/train_set_movies.json'\n",
    "dev_conversations_path = '../data/dev_set/dev_set_movies.json'\n",
    "\n",
    "\n",
    "# collect only samples where the answer is entity and it is adjacent to the seed entity\n",
    "train_dataset = []\n",
    "\n",
    "graph_sizes = []\n",
    "max_n_edges = 2409 # max size of the graph allowed in the number of edges\n",
    "\n",
    "\n",
    "rdfsLabelURI='http://www.w3.org/2000/01/rdf-schema#label'\n",
    "\n",
    "def lookup_entity_labels(entity_ids):\n",
    "    # prepare mapping tensors with entity labels and ids\n",
    "    entity_labels, s_entity_ids = [], []\n",
    "    for i, e_id in enumerate(entity_ids):\n",
    "        e_uri = kg.global_id_to_string(e_id, TripleComponentRole.OBJECT)\n",
    "        (triples, cardinality) = kg.search_triples(e_uri, rdfsLabelURI, \"\")\n",
    "        if cardinality > 0:\n",
    "            label = triples.next()[2]\n",
    "            # strip language marker\n",
    "            label = label.split('\"')[1]\n",
    "            entity_labels.append(label)\n",
    "            s_entity_ids.append(e_id)\n",
    "\n",
    "    assert len(entity_labels) == len(s_entity_ids)\n",
    "    return entity_labels, s_entity_ids\n",
    "\n",
    "\n",
    "def prepare_dataset(train_conversations_path, n_limit=NSAMPLES):\n",
    "    with open(train_conversations_path, \"r\") as data:\n",
    "            conversations = json.load(data)\n",
    "    print(\"%d conversations loaded\"%len(conversations))\n",
    "\n",
    "    # consider a sample of the dataset\n",
    "    if n_limit:\n",
    "        conversations = conversations[:n_limit]\n",
    "\n",
    "    n_entities = []\n",
    "    n_edges = []\n",
    "\n",
    "    train_dataset = []\n",
    "\n",
    "    for conversation in conversations[:NSAMPLES]:\n",
    "        # store history of the current conversation\n",
    "        dialogue_history = []\n",
    "        for i in range(len(conversation['questions'][:1])):\n",
    "\n",
    "            question = conversation['questions'][i]['question']\n",
    "            answer = conversation['questions'][i]['answer']\n",
    "            # use oracle for the correct initial entity\n",
    "            seed_entity = conversation['seed_entity'].split('/')[-1]\n",
    "            seed_entity_id = kg.string_to_global_id(PREFIX_E+seed_entity, TripleComponentRole.OBJECT)\n",
    "\n",
    "            # retrieve all adjacent nodes including literals\n",
    "            subgraph = kg.compute_hops([seed_entity_id], max_triples, offset)\n",
    "            entity_ids, predicate_ids, adjacencies = subgraph\n",
    "\n",
    "            assert len(predicate_ids) == len(adjacencies)\n",
    "        #         print(\"conversation\")\n",
    "\n",
    "            # answer literal\n",
    "            answer_label = answer\n",
    "            # consider only answers which are entities\n",
    "            if ('www.wikidata.org' in answer):\n",
    "                answer_id = kg.string_to_global_id(PREFIX_E+answer.split('/')[-1], TripleComponentRole.OBJECT)\n",
    "                in_subgraph = answer_id in entity_ids\n",
    "\n",
    "                # look up answer entity label\n",
    "                a_uri = PREFIX_E+answer.split('/')[-1]\n",
    "                (triples, cardinality) = kg.search_triples(a_uri, rdfsLabelURI, \"\")\n",
    "                if cardinality > 0:\n",
    "                    answer_label = triples.next()[2]\n",
    "                    # strip language marker\n",
    "                    answer_label = answer_label.split('\"')[1]\n",
    "\n",
    "                # retain samples with answer outside the seed subgraph\n",
    "                p_input_ids = []\n",
    "                p_token_type_ids = []\n",
    "                p_attention_masks = []\n",
    "\n",
    "                # prepare input of questions concatenated with all relation labels in the KG as candidates\n",
    "                # trim predicates\n",
    "                for p_label in all_predicate_labels:\n",
    "                    # encode a text pair of the question with a predicate label\n",
    "                    encoded_dict = tokenizer.encode_plus(question, p_label,\n",
    "                                                         add_special_tokens=True,\n",
    "                                                         max_length=32,\n",
    "                                                         pad_to_max_length=True,\n",
    "                                                         return_attention_mask=True,\n",
    "                                                         return_token_type_ids=True)\n",
    "                    p_input_ids.append(encoded_dict['input_ids'])\n",
    "                    p_token_type_ids.append(encoded_dict['token_type_ids'])\n",
    "                    p_attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "\n",
    "                # prepare input of questions concatenated with node labels as candidates: get labels for all candidate entities in the seed subgraph\n",
    "#                 entity_labels, entity_ids = lookup_entity_labels(entity_ids)\n",
    "                # create a batch of samples for each entity label separately\n",
    "                e_input_ids = []\n",
    "                e_token_type_ids = []\n",
    "                e_attention_masks = []\n",
    "#                 for e_label in entity_labels:\n",
    "#                     # encode a text pair of the question with a predicate label\n",
    "#                     encoded_dict = tokenizer.encode_plus([question]+dialogue_history[::-1], e_label,\n",
    "#                                                          add_special_tokens=True,\n",
    "#                                                          max_length=64,\n",
    "#                                                          pad_to_max_length=True,\n",
    "#                                                          return_attention_mask=True)\n",
    "#                     e_input_ids.append(encoded_dict['input_ids'])\n",
    "# #                     e_token_type_ids.append(encoded_dict['token_type_ids'])\n",
    "#                     e_attention_masks.append(encoded_dict['attention_mask'])\n",
    "#                 assert len(e_input_ids) == len(entity_ids)\n",
    "                first_question = None\n",
    "                if i == 0 and in_subgraph:\n",
    "#                     print('first question')\n",
    "                    first_question = torch.tensor([seed_entity_id])\n",
    "                    train_dataset.append([[torch.tensor(e_input_ids), torch.tensor(e_token_type_ids),\n",
    "                                           torch.tensor(e_attention_masks)],  # , torch.tensor(entity_ids)\n",
    "                                          [torch.tensor(p_input_ids), torch.tensor(p_token_type_ids),\n",
    "                                           torch.tensor(p_attention_masks)],  # , torch.tensor(all_predicate_ids)\n",
    "                                           torch.tensor([answer_id]), first_question])\n",
    "        \n",
    "#                     train_dataset.append([torch.tensor(p_input_ids),\n",
    "#                                           torch.tensor(p_token_type_ids),\n",
    "#                                           torch.tensor(p_attention_masks),\n",
    "#                                           first_question,\n",
    "#                                           torch.tensor([answer_idx])])\n",
    "            # carry over history to the next dialogue turn\n",
    "            dialogue_history.extend([question, answer_label])\n",
    "\n",
    "    del entity_ids, predicate_ids, adjacencies\n",
    "\n",
    "    print(\"Compiled dataset with %d samples\" % len(train_dataset))\n",
    "    return train_dataset\n",
    "\n",
    "\n",
    "train_dataset = prepare_dataset(train_conversations_path)\n",
    "valid_dataset = prepare_dataset(dev_conversations_path)\n",
    "\n",
    "# remove everything from memory but model and tensors for training/validaton\n",
    "kg.remove()\n",
    "del kg\n",
    "\n",
    "print(\"Dataset loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache() \n",
    "# del _\n",
    "memoryStats(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model (matching nodes and relations with a Transformer with subgraph sampling)\n",
    "n_batches = 1000\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "import sys\n",
    "import os\n",
    "import gc\n",
    "import psutil\n",
    "\n",
    "# training setup\n",
    "from transformers import get_linear_schedule_with_warmup, AdamW\n",
    "\n",
    "epochs = 4\n",
    "total_steps = len(train_dataset) * epochs\n",
    "\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-1, # args.learning_rate - default is 5e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8\n",
    "                 )\n",
    "# learning rate scheduler\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)\n",
    "\n",
    "print(\"%d training examples\"%(len(train_dataset)))\n",
    "print(\"%d validation examples\"%(len(valid_dataset)))\n",
    "\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "    \n",
    "    # reset the total loss for this epoch\n",
    "    total_train_loss = 0\n",
    "    \n",
    "    # put the model into training mode\n",
    "    model.train()\n",
    "    \n",
    "#     print(\"Started epoch\")\n",
    "#     memoryStats()\n",
    "    \n",
    "    # for each sample of training data input as a batch of size 1\n",
    "    n_losses = 0\n",
    "    for step, batch in enumerate(train_dataset[:n_batches]):\n",
    "#         print(step)\n",
    "        \n",
    "        e_inputs = [tensor.to(device) for tensor in batch[0]]\n",
    "        p_inputs = [tensor.to(device) for tensor in batch[1]]\n",
    "        labels = batch[2].to(device)\n",
    "        first_question = None\n",
    "        if batch[3]:\n",
    "            first_question = batch[3].to(device)\n",
    "        \n",
    "        model.zero_grad()\n",
    "        \n",
    "#         print(\"Sample ready\")\n",
    "#         memoryStats()\n",
    "        \n",
    "        # forward pass\n",
    "        loss, logits, entity_ids = model(e_inputs,\n",
    "                                         p_inputs,\n",
    "                                         labels,\n",
    "                                         first_question)\n",
    "        \n",
    "#         print(loss.item())\n",
    "        # accumulate the training loss over all of the batches\n",
    "        \n",
    "        \n",
    "#         print(\"Forward pass complete\")\n",
    "#         memoryStats()\n",
    "\n",
    "        # clean up\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        if not loss == None:\n",
    "            n_losses += 1\n",
    "            total_train_loss += float(loss.item())\n",
    "            \n",
    "#             memoryStats()\n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "        else:\n",
    "            total_train_loss += 10\n",
    "            \n",
    "#         for param in model.parameters():\n",
    "#             print(param.grad.data.sum())\n",
    "            \n",
    "#             print(\"Backprop complete\")\n",
    "#             memoryStats()\n",
    "        \n",
    "        # clip gradient to prevent exploding\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "#         weights = []\n",
    "#         for param in model.parameters(): # loop the weights in the model before updating and store them\n",
    "#             weights.append(param.clone())\n",
    "        \n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "#         weights_after_backprop = [] # weights after backprop\n",
    "#         for param in model.parameters():\n",
    "#             weights_after_backprop.append(param.clone()) # only layer1's weight should update, layer2 is not used\n",
    "\n",
    "#         for i in zip(weights, weights_after_backprop):\n",
    "#             if not torch.equal(i[0], i[1]):\n",
    "#                 print(\"Weights updated\")\n",
    "        \n",
    "        # clean up\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache() \n",
    "        \n",
    "    # training epoch is over here\n",
    "#     print(\"Training epoch complete\")\n",
    "#     memoryStats()\n",
    "    \n",
    "    # calculate average loss over all the batches\n",
    "    avg_train_loss = total_train_loss / len(train_dataset)\n",
    "    print(\"Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"Correct subgraphs selected:\", n_losses)\n",
    "    \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    # put the model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    total_eval_loss = 0\n",
    "\n",
    "    # evaluate data for one epoch\n",
    "    n_losses = 0\n",
    "    for step, batch in enumerate(valid_dataset[:n_batches]):\n",
    "#         print(step)\n",
    "        \n",
    "        e_inputs = [tensor.to(device) for tensor in batch[0]]\n",
    "        p_inputs = [tensor.to(device) for tensor in batch[1]]\n",
    "        labels = batch[2].to(device)\n",
    "        first_question = None\n",
    "        if batch[3]:\n",
    "            first_question = batch[3].to(device)\n",
    "        \n",
    "#         print(\"Sample ready\")\n",
    "#         memoryStats()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # forward pass\n",
    "            loss, logits, entity_ids = model(e_inputs,\n",
    "                                             p_inputs,\n",
    "                                             labels,\n",
    "                                             first_question)\n",
    "            \n",
    "            if not loss == None:\n",
    "                # accumulate validation loss\n",
    "                total_eval_loss += loss.item()\n",
    "                n_losses += 1\n",
    "            else:\n",
    "                total_eval_loss += 10\n",
    "\n",
    "#             print(\"Forward pass complete\")\n",
    "#             memoryStats()\n",
    "        \n",
    "        # clean up\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "#     print(\"Validation epoch complete\")\n",
    "#     memoryStats()\n",
    "\n",
    "    avg_val_loss = total_eval_loss / len(valid_dataset)\n",
    "    print(\"Average validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"Correct subgraphs selected:\", n_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(model, dataset, device):\n",
    "    # put model in evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # TODO add MRR\n",
    "    p1s, mrrs = [], []  # measure accuracy of the top answer: P@1\n",
    "    for batch in dataset:\n",
    "        e_inputs = [tensor.to(device) for tensor in batch[0]]\n",
    "        p_inputs = [tensor.to(device) for tensor in batch[1]]\n",
    "        labels = batch[2].to(device)\n",
    "        first_question = None\n",
    "        if batch[3]:\n",
    "            first_question = batch[3].to(device)\n",
    "#         print(first_question)\n",
    "        with torch.no_grad():\n",
    "            # forward pass\n",
    "            logits, entity_ids = model(e_inputs, p_inputs, first_question=first_question)\n",
    "            true_label = labels.cpu().numpy()[0]\n",
    "            if entity_ids and true_label in entity_ids:\n",
    "#                 print(len(logits.cpu().numpy()))\n",
    "#                 print(\"Correct subgraph\")\n",
    "                answer_idx = [entity_ids.index(true_label)]\n",
    "                print(answer_idx)\n",
    "                scores = logits.cpu().numpy()\n",
    "                rank = np.argsort(scores)[::-1]\n",
    "#                 print(rank[:5])\n",
    "                print(np.sort(scores)[::-1][:5])\n",
    "                predicted_label = rank[0]\n",
    "                p1 = int(predicted_label == true_label)\n",
    "                # position of the correct answer\n",
    "                position = np.where(rank == answer_idx)[0][0] + 1\n",
    "#                 print(position)\n",
    "                mrr = 1 / position\n",
    "#                 print(p1, mrr)\n",
    "            else:\n",
    "                p1 = mrr = 0\n",
    "                print(\"Incorrect subgraph\")\n",
    "            p1s.append(p1)\n",
    "            mrrs.append(mrr)\n",
    "    return p1s, mrrs\n",
    "\n",
    "p1s, mrrs = run_inference(model, train_dataset, device)\n",
    "print(\"Train set P@1: %.2f MRR: %.2f\" % (np.mean(p1s), np.mean(mrrs)))\n",
    "\n",
    "p1s, mrrs = run_inference(model, valid_dataset, device)\n",
    "print(\"Dev set P@1: %.2f MRR: %.2f\" % (np.mean(p1s), np.mean(mrrs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = './models/mpbert_%d/'\n",
    "# version = 0\n",
    "# output_dir = model_path % (version + epochs)\n",
    "\n",
    "# print(\"Saving model to %s\" % output_dir)\n",
    "\n",
    "\n",
    "# if not os.path.exists(output_dir):\n",
    "#     os.makedirs(output_dir)\n",
    "\n",
    "\n",
    "# model.save_pretrained(output_dir)\n",
    "# tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
