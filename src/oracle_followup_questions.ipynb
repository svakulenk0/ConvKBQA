{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not frozen Transformer layer\n",
      "transformer.layer.5.attention.q_lin.weight\n",
      "transformer.layer.5.attention.q_lin.bias\n",
      "transformer.layer.5.attention.k_lin.weight\n",
      "transformer.layer.5.attention.k_lin.bias\n",
      "transformer.layer.5.attention.v_lin.weight\n",
      "transformer.layer.5.attention.v_lin.bias\n",
      "transformer.layer.5.attention.out_lin.weight\n",
      "transformer.layer.5.attention.out_lin.bias\n",
      "transformer.layer.5.sa_layer_norm.weight\n",
      "transformer.layer.5.sa_layer_norm.bias\n",
      "transformer.layer.5.ffn.lin1.weight\n",
      "transformer.layer.5.ffn.lin1.bias\n",
      "transformer.layer.5.ffn.lin2.weight\n",
      "transformer.layer.5.ffn.lin2.bias\n",
      "transformer.layer.5.output_layer_norm.weight\n",
      "transformer.layer.5.output_layer_norm.bias\n",
      "Model loaded to cuda\n"
     ]
    }
   ],
   "source": [
    "# model init\n",
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertConfig\n",
    "\n",
    "from MPDistilBert_sampler_model import MessagePassingHDTBert\n",
    "\n",
    "DEVICE = 'cuda'\n",
    "\n",
    "# model configuration\n",
    "model_name = 'distilbert-base-uncased'\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "config = DistilBertConfig.from_pretrained(model_name, num_labels=1)\n",
    "\n",
    "E_BEAM = 10\n",
    "NSAMPLES = 100\n",
    "\n",
    "P_BEAM = 8000\n",
    "\n",
    "model = MessagePassingHDTBert(config, topk_entities=E_BEAM, topk_predicates=P_BEAM)\n",
    "\n",
    "for name, param in model.bert.named_parameters():                \n",
    "    if name.startswith('embeddings'):\n",
    "        param.requires_grad = False\n",
    "        \n",
    "# print(model.bert)\n",
    "\n",
    "# freeze only the first k-1 layers\n",
    "k = 6\n",
    "ct = 0\n",
    "for child in model.bert.transformer.layer.children():\n",
    "    ct += 1\n",
    "    if ct < k:\n",
    "        for param in child.parameters():\n",
    "            param.requires_grad = False\n",
    "    else:\n",
    "        print(\"Not frozen Transformer layer\")\n",
    "\n",
    "for name, param in model.bert.named_parameters():                \n",
    "    if param.requires_grad:\n",
    "        print(name)\n",
    "\n",
    "\n",
    "if DEVICE == 'cuda':\n",
    "    device = torch.device(\"cuda\")\n",
    "    model.cuda()\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Model loaded to\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph loaded\n"
     ]
    }
   ],
   "source": [
    "# load graph\n",
    "import json\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "\n",
    "from hdt import HDTDocument, TripleComponentRole\n",
    "\n",
    "from settings import *\n",
    "from predicates import properties\n",
    "\n",
    "\n",
    "hdt_file = 'wikidata2018_09_11.hdt'\n",
    "kg = HDTDocument(hdt_path+hdt_file)\n",
    "namespace = 'predef-wikidata2018-09-all'\n",
    "PREFIX_E = 'http://www.wikidata.org/entity/'\n",
    "PREFIX_P = 'http://www.wikidata.org/prop/'\n",
    "\n",
    "# prepare to retrieve all adjacent nodes including literals\n",
    "predicates_ids = []\n",
    "kg.configure_hops(1, predicates_ids, namespace, True, False)\n",
    "\n",
    "# load all predicate labels\n",
    "relationid2label = {}\n",
    "for p in properties['results']['bindings']:\n",
    "    _id = p['property']['value'].split('/')[-1]\n",
    "    label = p['propertyLabel']['value']\n",
    "    relationid2label[_id] = label\n",
    "\n",
    "# model init\n",
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertConfig\n",
    "\n",
    "from MPDistilBert_sampler_model import MessagePassingHDTBert\n",
    "from utils import adj\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# load all predicate labels\n",
    "from all_predicate_labels import all_predicate_labels\n",
    "\n",
    "print(\"Graph loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1344 conversations loaded\n",
      "Compiled dataset with 68 samples\n",
      "448 conversations loaded\n",
      "Compiled dataset with 48 samples\n",
      "Dataset loaded\n"
     ]
    }
   ],
   "source": [
    "# check how many times an answer to the question fall into the initial (seed) subgraph separately for each order in the question sequence\n",
    "max_triples = 50000\n",
    "offset = 0\n",
    "\n",
    "# dataset setup\n",
    "train_conversations_path = '../data/train_set/train_set_movies.json'\n",
    "dev_conversations_path = '../data/dev_set/dev_set_movies.json'\n",
    "\n",
    "\n",
    "# collect only samples where the answer is entity and it is adjacent to the seed entity\n",
    "train_dataset = []\n",
    "\n",
    "graph_sizes = []\n",
    "max_n_edges = 2409 # max size of the graph allowed in the number of edges\n",
    "\n",
    "\n",
    "rdfsLabelURI='http://www.w3.org/2000/01/rdf-schema#label'\n",
    "\n",
    "def lookup_entity_labels(entity_ids):\n",
    "    # prepare mapping tensors with entity labels and ids\n",
    "    entity_labels, s_entity_ids = [], []\n",
    "    for i, e_id in enumerate(entity_ids):\n",
    "        e_uri = kg.global_id_to_string(e_id, TripleComponentRole.OBJECT)\n",
    "        (triples, cardinality) = kg.search_triples(e_uri, rdfsLabelURI, \"\")\n",
    "        if cardinality > 0:\n",
    "            label = triples.next()[2]\n",
    "            # strip language marker\n",
    "            label = label.split('\"')[1]\n",
    "            entity_labels.append(label)\n",
    "            s_entity_ids.append(e_id)\n",
    "\n",
    "    assert len(entity_labels) == len(s_entity_ids)\n",
    "    return entity_labels, s_entity_ids\n",
    "\n",
    "\n",
    "def get_answer(answer, kg):\n",
    "    # answer literal\n",
    "    answer_label = answer\n",
    "    answer_id = None\n",
    "    if ('www.wikidata.org' in answer):\n",
    "        # look up answer entity label\n",
    "        a_uri = PREFIX_E+answer.split('/')[-1]\n",
    "        (triples, cardinality) = kg.search_triples(a_uri, rdfsLabelURI, \"\")\n",
    "        if cardinality > 0:\n",
    "            answer_label = triples.next()[2]\n",
    "            # strip language marker\n",
    "            answer_label = answer_label.split('\"')[1]\n",
    "        answer_id = kg.string_to_global_id(PREFIX_E+answer.split('/')[-1], TripleComponentRole.OBJECT)\n",
    "    return answer_label, answer_id\n",
    "\n",
    "\n",
    "def prepare_dataset(train_conversations_path, n_limit=NSAMPLES):\n",
    "    with open(train_conversations_path, \"r\") as data:\n",
    "            conversations = json.load(data)\n",
    "    print(\"%d conversations loaded\"%len(conversations))\n",
    "\n",
    "    # consider a sample of the dataset\n",
    "    if n_limit:\n",
    "        conversations = conversations[:n_limit]\n",
    "\n",
    "    train_dataset = []\n",
    "\n",
    "    for conversation in conversations[:NSAMPLES]:\n",
    "        \n",
    "        # 1st question oracle\n",
    "        question = conversation['questions'][0]['question']\n",
    "        seed_entity = conversation['seed_entity'].split('/')[-1]\n",
    "        seed_entity_id = kg.string_to_global_id(PREFIX_E+seed_entity, TripleComponentRole.OBJECT)\n",
    "        pivot_entities = [seed_entity_id]\n",
    "        \n",
    "        answer = conversation['questions'][0]['answer']\n",
    "        answer_label, answer_id = get_answer(answer, kg)\n",
    "        if answer_id:\n",
    "            pivot_entities.append(answer_id)\n",
    "        \n",
    "        # store history of the current conversation\n",
    "        dialogue_history = [question, answer_label]\n",
    "        \n",
    "        # consider follow-up questions: all 2nd questions here\n",
    "        for i in range(len(conversation['questions']))[1:2]:\n",
    "            question = conversation['questions'][i]['question']\n",
    "            \n",
    "            answer = conversation['questions'][i]['answer']\n",
    "            answer_label, answer_id = get_answer(answer, kg)\n",
    "                \n",
    "            # consider only answers which are entities\n",
    "            if ('www.wikidata.org' in answer):\n",
    "#                 print(dialogue_history)\n",
    "#                 print(question)\n",
    "                \n",
    "                # retrieve all adjacent nodes including literals\n",
    "                subgraph = kg.compute_hops(pivot_entities, max_triples, offset)\n",
    "                entity_ids, predicate_ids, adjacencies = subgraph\n",
    "                assert len(predicate_ids) == len(adjacencies)\n",
    "                \n",
    "\n",
    "                # retain samples with answer outside the seed subgraph\n",
    "                p_input_ids = []\n",
    "                p_token_type_ids = []\n",
    "                p_attention_masks = []\n",
    "                \n",
    "\n",
    "                # prepare input of questions concatenated with all relation labels in the KG as candidates\n",
    "                # trim predicates\n",
    "                for p_label in all_predicate_labels:\n",
    "                    # encode a text pair of the question with a predicate label\n",
    "                    encoded_dict = tokenizer.encode_plus(question, p_label,\n",
    "                                                         add_special_tokens=True,\n",
    "                                                         max_length=32,\n",
    "                                                         pad_to_max_length=True,\n",
    "                                                         return_attention_mask=True,\n",
    "                                                         return_token_type_ids=True)\n",
    "                    p_input_ids.append(encoded_dict['input_ids'])\n",
    "                    p_token_type_ids.append(encoded_dict['token_type_ids'])\n",
    "                    p_attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "\n",
    "                # prepare input of questions concatenated with node labels as candidates: get labels for all candidate entities in the seed subgraph\n",
    "                entity_labels, entity_ids = lookup_entity_labels(entity_ids)\n",
    "                # create a batch of samples for each entity label separately\n",
    "                e_input_ids = []\n",
    "                e_token_type_ids = []\n",
    "                e_attention_masks = []\n",
    "                for e_label in entity_labels:\n",
    "                    # encode a text pair of the question with a predicate label\n",
    "                    encoded_dict = tokenizer.encode_plus([question]+dialogue_history[::-1], e_label,\n",
    "                                                         add_special_tokens=True,\n",
    "                                                         max_length=64,\n",
    "                                                         pad_to_max_length=True,\n",
    "                                                         return_attention_mask=True)\n",
    "                    e_input_ids.append(encoded_dict['input_ids'])\n",
    "#                     e_token_type_ids.append(encoded_dict['token_type_ids'])\n",
    "                    e_attention_masks.append(encoded_dict['attention_mask'])\n",
    "                assert len(e_input_ids) == len(entity_ids)\n",
    "                train_dataset.append([[torch.tensor(e_input_ids), torch.tensor(e_token_type_ids),\n",
    "                                       torch.tensor(e_attention_masks), torch.tensor(entity_ids)],\n",
    "                                      [torch.tensor(p_input_ids), torch.tensor(p_token_type_ids),\n",
    "                                       torch.tensor(p_attention_masks)],  # , torch.tensor(all_predicate_ids)\n",
    "                                       torch.tensor([answer_id])])\n",
    "        \n",
    "            # carry over history to the next dialogue turn\n",
    "            dialogue_history.extend([question, answer_label])\n",
    "\n",
    "    del entity_ids, predicate_ids, adjacencies\n",
    "\n",
    "    print(\"Compiled dataset with %d samples\" % len(train_dataset))\n",
    "    return train_dataset\n",
    "\n",
    "\n",
    "train_dataset = prepare_dataset(train_conversations_path)\n",
    "valid_dataset = prepare_dataset(dev_conversations_path)\n",
    "\n",
    "\n",
    "print(\"Dataset loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu % 8.5\n",
      "memory used GB: 2.74\n",
      "available gpu: 22.38\n",
      "used gpu: 0.25\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "\n",
    "# remove everything from memory but model and tensors for training/validaton\n",
    "kg.remove()\n",
    "del kg\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache() \n",
    "# del _\n",
    "memoryStats(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 training examples\n",
      "48 validation examples\n",
      "\n",
      "======== Epoch 1 / 3 ========\n",
      "Training...\n",
      "cpu % 8.5\n",
      "memory used GB: 2.95\n",
      "available gpu: 22.38\n",
      "used gpu: 9.40\n",
      "cpu % 8.9\n",
      "memory used GB: 2.96\n",
      "available gpu: 22.38\n",
      "used gpu: 11.37\n",
      "cpu % 10.0\n",
      "memory used GB: 2.96\n",
      "available gpu: 22.38\n",
      "used gpu: 9.65\n",
      "cpu % 8.5\n",
      "memory used GB: 2.96\n",
      "available gpu: 22.38\n",
      "used gpu: 10.66\n",
      "cpu % 8.5\n",
      "memory used GB: 2.96\n",
      "available gpu: 22.38\n",
      "used gpu: 9.84\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 5.71 GiB (GPU 0; 22.38 GiB total capacity; 17.51 GiB already allocated; 4.20 GiB free; 17.65 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-031b9455364c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;31m# forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0me_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;31m# clean up\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/transformers/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ConvKBQA/src/MPDistilBert_sampler_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, entities_q, predicates_q, answer, first_question)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;31m#         input_ids, token_type_ids, attention_mask, entity_ids = entities_q\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;31m#         print(\"Ranking %d entities\"%(len(entities_q[0])))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         e_outputs = self.bert(\n\u001b[0m\u001b[1;32m    218\u001b[0m             \u001b[0mentities_q\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mentities_q\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/transformers/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/transformers/lib/python3.8/site-packages/transformers/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, seq_length, dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m         \u001b[0mtfmr_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m         \u001b[0mhidden_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfmr_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/transformers/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/transformers/lib/python3.8/site-packages/transformers/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_embeddings\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mposition_embeddings\u001b[0m  \u001b[0;31m# (bs, max_seq_length, dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, max_seq_length, dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, max_seq_length, dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/transformers/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/transformers/lib/python3.8/site-packages/torch/nn/modules/normalization.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         return F.layer_norm(\n\u001b[0m\u001b[1;32m    153\u001b[0m             input, self.normalized_shape, self.weight, self.bias, self.eps)\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/transformers/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   1693\u001b[0m     \u001b[0mSee\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdetails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m     \"\"\"\n\u001b[0;32m-> 1695\u001b[0;31m     return torch.layer_norm(input, normalized_shape, weight, bias, eps,\n\u001b[0m\u001b[1;32m   1696\u001b[0m                             torch.backends.cudnn.enabled)\n\u001b[1;32m   1697\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 5.71 GiB (GPU 0; 22.38 GiB total capacity; 17.51 GiB already allocated; 4.20 GiB free; 17.65 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "# train model (matching nodes and relations with a Transformer with subgraph sampling)\n",
    "n_batches = 1000\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "import sys\n",
    "import os\n",
    "import gc\n",
    "import psutil\n",
    "\n",
    "# training setup\n",
    "from transformers import get_linear_schedule_with_warmup, AdamW\n",
    "\n",
    "epochs = 3\n",
    "total_steps = len(train_dataset) * epochs\n",
    "\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-1, # args.learning_rate - default is 5e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8\n",
    "                 )\n",
    "# learning rate scheduler\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)\n",
    "\n",
    "print(\"%d training examples\"%(len(train_dataset)))\n",
    "print(\"%d validation examples\"%(len(valid_dataset)))\n",
    "\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "    \n",
    "    # reset the total loss for this epoch\n",
    "    total_train_loss = 0\n",
    "    \n",
    "    # put the model into training mode\n",
    "    model.train()\n",
    "    \n",
    "    # for each sample of training data input as a batch of size 1\n",
    "    n_losses = 0\n",
    "    for step, batch in enumerate(train_dataset[:n_batches]):\n",
    "        e_inputs = [tensor.to(device) for tensor in batch[0]]\n",
    "        p_inputs = [tensor.to(device) for tensor in batch[1]]\n",
    "        labels = batch[2].to(device)\n",
    "        \n",
    "        model.zero_grad()\n",
    "    \n",
    "        # forward pass\n",
    "        loss, _, _ = model(e_inputs, p_inputs, labels)\n",
    "        del e_inputs, p_inputs, labels, _\n",
    "        # clean up\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        if not loss == None:\n",
    "            n_losses += 1\n",
    "            total_train_loss += float(loss.item())\n",
    "            \n",
    "            memoryStats(DEVICE)\n",
    "\n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "        else:\n",
    "            total_train_loss += 10\n",
    "        \n",
    "        # clip gradient to prevent exploding\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        \n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # clean up\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache() \n",
    "        \n",
    "    # training epoch is over here\n",
    "    # calculate average loss over all the batches\n",
    "    avg_train_loss = total_train_loss / len(train_dataset)\n",
    "    print(\"Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"Correct subgraphs selected:\", n_losses)\n",
    "    \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    # put the model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    total_eval_loss = 0\n",
    "\n",
    "    # evaluate data for one epoch\n",
    "    n_losses = 0\n",
    "    for step, batch in enumerate(valid_dataset[:n_batches]):\n",
    "#         print(step)\n",
    "        \n",
    "        e_inputs = [tensor.to(device) for tensor in batch[0]]\n",
    "        p_inputs = [tensor.to(device) for tensor in batch[1]]\n",
    "        labels = batch[2].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # forward pass\n",
    "            loss, _, _ = model(e_inputs,\n",
    "                               p_inputs,\n",
    "                               labels)\n",
    "            \n",
    "            del _, labels, p_inputs, e_inputs\n",
    "            \n",
    "            # clean up\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            if not loss == None:\n",
    "                # accumulate validation loss\n",
    "                total_eval_loss += loss.item()\n",
    "                n_losses += 1\n",
    "            else:\n",
    "                total_eval_loss += 10\n",
    "\n",
    "    avg_val_loss = total_eval_loss / len(valid_dataset)\n",
    "    print(\"Average validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"Correct subgraphs selected:\", n_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(model, dataset, device):\n",
    "    # put model in evaluation mode\n",
    "    model.eval()\n",
    "    correct_subgraphs = 0\n",
    "    # TODO add MRR\n",
    "    p1s, mrrs = [], []  # measure accuracy of the top answer: P@1\n",
    "    for batch in dataset:\n",
    "        e_inputs = [tensor.to(device) for tensor in batch[0]]\n",
    "        p_inputs = [tensor.to(device) for tensor in batch[1]]\n",
    "        labels = batch[2].to(device)\n",
    "        first_question = None\n",
    "#         if batch[3]:\n",
    "#             first_question = batch[3].to(device)\n",
    "#         print(first_question)\n",
    "        with torch.no_grad():\n",
    "            # forward pass\n",
    "            logits, entity_ids = model(e_inputs, p_inputs, first_question=first_question)\n",
    "            true_label = labels.cpu().numpy()[0]\n",
    "            if entity_ids and true_label in entity_ids:\n",
    "                correct_subgraphs += 1\n",
    "#                 print(len(logits.cpu().numpy()))\n",
    "#                 print(\"Correct subgraph\")\n",
    "                answer_idx = [entity_ids.index(true_label)]\n",
    "#                 print(answer_idx)\n",
    "                scores = logits.cpu().numpy()\n",
    "                rank = np.argsort(scores)[::-1]\n",
    "#                 print(rank[:5])\n",
    "#                 print(np.sort(scores)[::-1][:5])\n",
    "                predicted_label = rank[0]\n",
    "                p1 = int(predicted_label == true_label)\n",
    "                # position of the correct answer\n",
    "                position = np.where(rank == answer_idx)[0][0] + 1\n",
    "#                 print(position)\n",
    "                mrr = 1 / position\n",
    "#                 print(p1, mrr)\n",
    "            else:\n",
    "                p1 = mrr = 0\n",
    "#                 print(\"Incorrect subgraph\")\n",
    "            p1s.append(p1)\n",
    "            mrrs.append(mrr)\n",
    "    return p1s, mrrs, correct_subgraphs/len(dataset)\n",
    "\n",
    "p1s, mrrs, subgraphs_hit = run_inference(model, train_dataset, device)\n",
    "print(\"Train set P@1: %.2f MRR: %.2f subgraphs %.2f\" % (np.mean(p1s), np.mean(mrrs), subgraphs_hit))\n",
    "\n",
    "p1s, mrrs, subgraphs_hit = run_inference(model, valid_dataset, device)\n",
    "print(\"Dev set P@1: %.2f MRR: %.2f subgraphs %.2f\" % (np.mean(p1s), np.mean(mrrs), subgraphs_hit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = './models/mpbert_%d/'\n",
    "# version = 0\n",
    "# output_dir = model_path % (version + epochs)\n",
    "\n",
    "# print(\"Saving model to %s\" % output_dir)\n",
    "\n",
    "\n",
    "# if not os.path.exists(output_dir):\n",
    "#     os.makedirs(output_dir)\n",
    "\n",
    "\n",
    "# model.save_pretrained(output_dir)\n",
    "# tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
