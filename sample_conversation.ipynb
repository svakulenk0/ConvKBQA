{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6720 conversations loaded\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "train_conversations_path = './data/train_set/train_set_ALL.json'\n",
    "PREFIX_E = 'http://www.wikidata.org/entity/'\n",
    "\n",
    "# load training set\n",
    "with open(train_conversations_path, \"r\") as data:\n",
    "    conversations = json.load(data)\n",
    "print(\"%d conversations loaded\"%len(conversations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which famous author wrote the fantasy book series Harry Potter?\n",
      "https://www.wikidata.org/wiki/Q8337\n",
      "\n",
      "\n",
      "Which famous author wrote the fantasy book series Harry Potter?\n",
      "https://www.wikidata.org/wiki/Q34660\n",
      "Which book was the first one written?\n",
      "https://www.wikidata.org/wiki/Q43361\n",
      "What novel was the final one?\n",
      "https://www.wikidata.org/wiki/Q46758\n",
      "What character joins Harry Potter after being saved by him?\n",
      "https://www.wikidata.org/wiki/Q174009\n",
      "The series consists of which amount of books?\n",
      "\n",
      "\n",
      "5 questions 4 answer-entities\n"
     ]
    }
   ],
   "source": [
    "# load a sample conversation as a sequence of entities\n",
    "conversation = conversations[4]\n",
    "question = conversation['questions'][0]['question']\n",
    "print(question)\n",
    "\n",
    "entities = []\n",
    "seed_entity = conversation['seed_entity']\n",
    "entities.append(PREFIX_E + seed_entity.split('/')[-1])\n",
    "print(seed_entity)\n",
    "print('\\n')\n",
    "\n",
    "n_questions = len(conversation['questions'])\n",
    "for i in range(n_questions):\n",
    "    question = conversation['questions'][i]['question']\n",
    "    print(question)\n",
    "    \n",
    "    answer = conversation['questions'][i]['answer']\n",
    "    # consider only answers which are entities\n",
    "    if ('www.wikidata.org' in answer): \n",
    "        print(answer)\n",
    "        entity = PREFIX_E + answer.split('/')[-1]\n",
    "        entities.append(entity)\n",
    "\n",
    "print('\\n')    \n",
    "print(\"%d questions %d answer-entities\"%(n_questions, len(entities)-1))\n",
    "\n",
    "# intermediate entities: Karen Carpenter https://www.wikidata.org/wiki/Q1250861\n",
    "# TODO retrieve relations between these entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve Relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load KG\n",
    "from hdt import HDTDocument, TripleComponentRole\n",
    "from settings import *\n",
    "\n",
    "hdt_file = 'wikidata2018_09_11.hdt'\n",
    "kg = HDTDocument(hdt_path+hdt_file)\n",
    "\n",
    "max_triples = 50000\n",
    "offset = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50438799, 25203200, 34545680, 38167359, 7397250]\n"
     ]
    }
   ],
   "source": [
    "# look up entity ids in the KG\n",
    "matched_entity_ids = []\n",
    "for entity in entities:\n",
    "    matched_entity_ids.append(kg.string_to_global_id(entity, TripleComponentRole.OBJECT))\n",
    "print(matched_entity_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subgraph with 4153 entities and 415 relation types\n"
     ]
    }
   ],
   "source": [
    "# retrieve relevant subgraph\n",
    "subgraph = kg.compute_hops(matched_entity_ids, max_triples, offset)\n",
    "entity_ids, predicate_ids, adjacencies = subgraph\n",
    "\n",
    "n_entities = len(entity_ids)\n",
    "n_relations = len(predicate_ids)\n",
    "\n",
    "assert n_relations == len(adjacencies)\n",
    "\n",
    "print(\"Subgraph with %d entities and %d relation types\"%(n_entities, n_relations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 entities activated\n"
     ]
    }
   ],
   "source": [
    "# activate matched entities\n",
    "row, col, data = [], [], []\n",
    "score = 1\n",
    "for i, e in enumerate(matched_entity_ids):\n",
    "    idx = entity_ids.index(e)\n",
    "    row.append(i)\n",
    "    col.append(idx)\n",
    "    data.append(score)\n",
    "x = sp.csr_matrix((data, (row, col)), shape=(len(matched_entity_ids), n_entities))\n",
    "print(\"%d entities activated\"%len(matched_entity_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "415 adjacency matrices for each of the relation types\n"
     ]
    }
   ],
   "source": [
    "# load adjacencies\n",
    "def generate_adj_sp(adjacencies, n_entities, include_inverse):\n",
    "    '''\n",
    "    Build adjacency matrix\n",
    "    '''\n",
    "    adj_shape = (n_entities, n_entities)\n",
    "    \n",
    "    # colect all predicate matrices separately into a list\n",
    "    sp_adjacencies = []\n",
    "    for edges in adjacencies:\n",
    "        \n",
    "        # split subject (row) and object (col) node URIs\n",
    "        n_edges = len(edges)\n",
    "        row, col = np.transpose(edges)\n",
    "        \n",
    "        # duplicate edges in the opposite direction\n",
    "        if include_inverse:\n",
    "            _row = np.hstack([row, col])\n",
    "            col = np.hstack([col, row])\n",
    "            row = _row\n",
    "            n_edges *= 2\n",
    "        \n",
    "        # create adjacency matrix for this predicate\n",
    "        data = np.ones(n_edges)\n",
    "        adj = sp.csr_matrix((data, (row, col)), shape=adj_shape)\n",
    "        sp_adjacencies.append(adj)\n",
    "    \n",
    "    return np.asarray(sp_adjacencies)\n",
    "\n",
    "A = generate_adj_sp(adjacencies, n_entities, include_inverse=True)\n",
    "print(\"%d adjacency matrices for each of the relation types\" % len(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MP separately for each relation type\n",
    "from sklearn.preprocessing import normalize, binarize\n",
    "\n",
    "y = sp.csr_matrix((len(matched_entity_ids), n_entities))\n",
    "\n",
    "for _A in A:\n",
    "    _y = x @ _A\n",
    "    # normalize: cut top to 1\n",
    "#     _y[_y > 1] = 1\n",
    "    y += _y\n",
    "    \n",
    "sum_a = sum(y)\n",
    "sum_a_norm = sum_a.toarray()[0] / 2\n",
    "# normalize: cut top to 1\n",
    "# sum_a_norm[sum_a_norm > 1] = 1\n",
    "# activations across components\n",
    "y_counts = binarize(y, threshold=0.0)\n",
    "count_a = sum(y_counts).toarray()[0]\n",
    "# final scores\n",
    "y = (sum_a_norm + count_a) / (2 + 1)\n",
    "\n",
    "# check output size\n",
    "assert y.shape[0] == n_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.wikidata.org/entity/Q8337\n",
      "http://www.wikidata.org/entity/Q34660\n",
      "http://www.wikidata.org/entity/Q43361\n",
      "http://www.wikidata.org/entity/Q46758\n",
      "http://www.wikidata.org/entity/Q174009\n"
     ]
    }
   ],
   "source": [
    "# TODO find all edges between the matched nodes\n",
    "# there is an edge to the node if it remains activated after MP\n",
    "top = np.argwhere(y > 0).T.tolist()[0]\n",
    "if len(top) > 0:\n",
    "    activated_ids = np.asarray(entity_ids)[top]\n",
    "    answer_uris = []\n",
    "    for a in activated_ids:\n",
    "        uri = kg.global_id_to_string(a, TripleComponentRole.SUBJECT)\n",
    "        if uri:\n",
    "            answer_uris.append(uri)\n",
    "            if uri in entities:\n",
    "                print(uri)\n",
    "\n",
    "# directly connected matched nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.wikidata.org/entity/Q8337\n",
      "http://www.wikidata.org/entity/Q34660\n",
      "http://www.wikidata.org/entity/Q43361\n",
      "http://www.wikidata.org/entity/Q46758\n",
      "http://www.wikidata.org/entity/Q174009\n",
      "http://www.wikidata.org/entity/Q5410773\n",
      "http://www.wikidata.org/entity/Q190125\n",
      "http://www.wikidata.org/entity/Q568642\n",
      "http://www.wikidata.org/entity/Q452283\n",
      "http://www.wikidata.org/entity/Q3244512\n",
      "http://www.wikidata.org/entity/Q102438\n",
      "http://www.wikidata.org/entity/Q161678\n",
      "http://www.wikidata.org/entity/Q20711488\n",
      "http://www.wikidata.org/entity/Q216930\n",
      "http://www.wikidata.org/entity/Q232009\n",
      "http://www.wikidata.org/entity/Q1250951\n",
      "http://www.wikidata.org/entity/Q173998\n",
      "http://www.wikidata.org/entity/Q176132\n",
      "http://www.wikidata.org/entity/Q176772\n",
      "http://www.wikidata.org/entity/Q177439\n",
      "http://www.wikidata.org/entity/Q179641\n",
      "http://www.wikidata.org/entity/Q187923\n",
      "http://www.wikidata.org/entity/Q190366\n",
      "http://www.wikidata.org/entity/Q192179\n",
      "http://www.wikidata.org/entity/Q27924622\n",
      "http://www.wikidata.org/entity/Q3741059\n",
      "http://www.wikidata.org/entity/Q3744404\n",
      "http://www.wikidata.org/entity/Q712548\n",
      "http://www.wikidata.org/entity/Q713701\n",
      "http://www.wikidata.org/entity/Q717594\n",
      "http://www.wikidata.org/entity/Q845210\n",
      "http://www.wikidata.org/entity/Q46887\n",
      "http://www.wikidata.org/entity/Q47209\n",
      "http://www.wikidata.org/entity/Q145\n",
      "http://www.wikidata.org/entity/Q46751\n",
      "http://www.wikidata.org/entity/Q47598\n",
      "http://www.wikidata.org/entity/Q80817\n",
      "http://www.wikidata.org/entity/Q102235\n",
      "http://www.wikidata.org/entity/Q102244\n",
      "http://www.wikidata.org/entity/Q102448\n",
      "http://www.wikidata.org/entity/Q7979\n"
     ]
    }
   ],
   "source": [
    "# find entities that were activated more than once\n",
    "top = np.argwhere(y > 1).T.tolist()[0]\n",
    "if len(top) > 0:\n",
    "    activated_ids = np.asarray(entity_ids)[top]\n",
    "    answer_uris = []\n",
    "    for a in activated_ids:\n",
    "        uri = kg.global_id_to_string(a, TripleComponentRole.SUBJECT)\n",
    "        if uri:\n",
    "            answer_uris.append(uri)\n",
    "            print(uri)\n",
    "\n",
    "# those are the entities located at the intersection of the matched nodes\n",
    "# TODO explain relations to these nodes\n",
    "# TODO are they necessary to connect the matched nodes or is there already an edge between this pair of nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get connection for each of these entities to each of the matched entities"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
