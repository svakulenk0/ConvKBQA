{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2240 conversations loaded\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "split = 'dev_set'\n",
    "\n",
    "train_conversations_path = './data/%s/%s_ALL.json' % (split, split)\n",
    "PREFIX_E = 'http://www.wikidata.org/entity/'\n",
    "\n",
    "# load training set\n",
    "with open(train_conversations_path, \"r\") as data:\n",
    "    conversations = json.load(data)\n",
    "print(\"%d conversations loaded\"%len(conversations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load KG\n",
    "import re\n",
    "\n",
    "from hdt import HDTDocument, TripleComponentRole\n",
    "from settings import *\n",
    "\n",
    "hdt_file = 'wikidata20200309.hdt'\n",
    "kg = HDTDocument(hdt_path+hdt_file)\n",
    "namespace = 'predef-wikidata2020-03-all'\n",
    "predicates_ids = []\n",
    "kg.configure_hops(1, predicates_ids, namespace, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "# load a sample conversation as a sequence of entities\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from collections import Counter\n",
    "\n",
    "subgraph_entities = []\n",
    "subgraph_relations = []\n",
    "\n",
    "def retrieve_subgraph(kg, matched_entity_ids, max_triples=500000000, offset=0):\n",
    "    entity_ids, predicate_ids, adjacencies = [], [], []\n",
    "    while True:\n",
    "        _entity_ids, _predicate_ids, _adjacencies = kg.compute_hops(matched_entity_ids, max_triples, offset)\n",
    "        if not _entity_ids:\n",
    "            return entity_ids, predicate_ids, adjacencies\n",
    "        # accumulate all splits\n",
    "        entity_ids.extend(_entity_ids)\n",
    "        predicate_ids.extend(_predicate_ids)\n",
    "        adjacencies.extend(_adjacencies)\n",
    "        offset += max_triples\n",
    "\n",
    "\n",
    "for j, conversation in enumerate(conversations[:]):\n",
    "    print(j)\n",
    "    seed_entity = PREFIX_E + conversation['seed_entity'].split('/')[-1]\n",
    "    seed_entity_text = conversation['seed_entity_text']\n",
    "    seed_entity_id = kg.string_to_global_id(seed_entity, TripleComponentRole.OBJECT)\n",
    "    matched_entity_ids = [seed_entity_id]\n",
    "    n_questions = len(conversation['questions'])\n",
    "    \n",
    "    questions = []\n",
    "    answer_entities = []\n",
    "    answer_texts = []\n",
    "    answer_ids = []\n",
    "    for i in range(n_questions):\n",
    "        question = conversation['questions'][i]['question']\n",
    "        questions.append(question)\n",
    "        \n",
    "        answers = conversation['questions'][i]['answer'].split(';')\n",
    "        answer_text = conversation['questions'][i]['answer_text']\n",
    "        answer_texts.append(answer_text)\n",
    "\n",
    "        _answer_entities = []\n",
    "        _answer_ids = []\n",
    "        for answer in answers:\n",
    "            # consider only answers which are entities\n",
    "            if ('www.wikidata.org' in answer):\n",
    "                entity = PREFIX_E + answer.split('/')[-1]\n",
    "                _answer_entities.append(entity)\n",
    "                _answer_ids.append(kg.string_to_global_id(entity, TripleComponentRole.OBJECT))\n",
    "        \n",
    "        answer_entities.append(_answer_entities)\n",
    "        answer_ids.append(_answer_ids)\n",
    "    \n",
    "    matched_entity_ids.extend([a for _as in answer_ids for a in _as if a])\n",
    "    \n",
    "    # retrieve relevant subgraph\n",
    "    entity_ids, predicate_ids, adjacencies = retrieve_subgraph(kg, matched_entity_ids)\n",
    "    \n",
    "    subgraph_entities.append(len(entity_ids))\n",
    "    subgraph_relations.append(len(predicate_ids))\n",
    "    \n",
    "    # dump sample with subgraph as json\n",
    "    data = {'seed_entity': seed_entity, 'seed_entity_text': seed_entity_text, 'seed_entity_id': seed_entity_id,\n",
    "            'questions': questions,\n",
    "            'answer_entities': answer_entities, 'answer_texts': answer_texts, 'answer_ids': answer_ids,\n",
    "            'entities': entity_ids, 'predicates': predicate_ids, 'adjacencies': adjacencies}\n",
    "    json_object = json.dumps(data)\n",
    "    with open('./data/subgraphs/%s/%d.json' % (split, j), \"w\") as outfile:\n",
    "        outfile.write(json_object) \n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats(_list):\n",
    "    print(min(_list), np.mean(_list), max(_list))\n",
    "    print(Counter(_list))\n",
    "    print('\\n')\n",
    "\n",
    "stats(subgraph_entities)\n",
    "stats(subgraph_relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
