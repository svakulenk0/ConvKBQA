{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6720 conversations loaded\n"
     ]
    }
   ],
   "source": [
    "# load graph\n",
    "import json\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "\n",
    "from hdt import HDTDocument, TripleComponentRole\n",
    "\n",
    "from settings import *\n",
    "from predicates import properties\n",
    "\n",
    "\n",
    "hdt_file = 'wikidata2018_09_11.hdt'\n",
    "kg = HDTDocument(hdt_path+hdt_file)\n",
    "namespace = 'predef-wikidata2018-09-all'\n",
    "PREFIX_E = 'http://www.wikidata.org/entity/'\n",
    "\n",
    "# prepare to retrieve all adjacent nodes including literals\n",
    "predicates_ids = []\n",
    "kg.configure_hops(1, predicates_ids, namespace, True, False)\n",
    "\n",
    "# load all predicate labels\n",
    "\n",
    "relationid2label = {}\n",
    "for p in properties['results']['bindings']:\n",
    "    _id = p['property']['value'].split('/')[-1]\n",
    "    label = p['propertyLabel']['value']\n",
    "    relationid2label[_id] = label\n",
    "    \n",
    "def check_answer_in_subgraph(answer, entity_ids):\n",
    "    # consider only answers which are entities\n",
    "    if ('www.wikidata.org' in answer):\n",
    "        answer_id = kg.string_to_global_id(PREFIX_E+answer.split('/')[-1], TripleComponentRole.OBJECT)\n",
    "        in_subgraph = answer_id in entity_ids\n",
    "        # consider only answer entities that are in the subgraph\n",
    "        if in_subgraph:\n",
    "            answer_idx = entity_ids.index(answer_id)\n",
    "            return answer_idx\n",
    "\n",
    "# load the training dataset\n",
    "train_conversations_path = './data/train_set/train_set_ALL.json'\n",
    "\n",
    "with open(train_conversations_path, \"r\") as data:\n",
    "        conversations = json.load(data)\n",
    "print(\"%d conversations loaded\"%len(conversations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 4320, 1: 3424, 2: 3104, 3: 3104, 4: 2464})\n",
      "Min: 37 Mean: 700.87 Max: 12605\n"
     ]
    }
   ],
   "source": [
    "# check how many times an answer to the question fall into the initial (seed) subgraph separately for each order in the question sequence\n",
    "\n",
    "max_triples = 50000000\n",
    "offset = 0\n",
    "\n",
    "# collect only samples where the answer is entity and it is adjacent to the seed entity\n",
    "train_dataset = []\n",
    "\n",
    "graph_sizes = []\n",
    "max_n_edges = 2409 # max size of the graph allowed in the number of edges\n",
    "\n",
    "# consider a sample of the dataset\n",
    "n_limit = None\n",
    "if n_limit:\n",
    "    conversations = conversations[:n_limit]\n",
    "\n",
    "counts = Counter()\n",
    "n_entities = []\n",
    "for conversation in conversations:\n",
    "    for i in range(len(conversation['questions'])):\n",
    "        question = conversation['questions'][i]['question']\n",
    "        answer = conversation['questions'][i]['answer']\n",
    "        # use oracle for the correct initial entity\n",
    "        seed_entity = conversation['seed_entity'].split('/')[-1]\n",
    "        seed_entity_id = kg.string_to_global_id(PREFIX_E+seed_entity, TripleComponentRole.OBJECT)\n",
    "\n",
    "        # retrieve all adjacent nodes including literals\n",
    "        subgraph = kg.compute_hops([seed_entity_id], max_triples, offset)\n",
    "        entity_ids, predicate_ids, adjacencies = subgraph\n",
    "        assert len(predicate_ids) == len(adjacencies)\n",
    "    #         print(\"conversation\")\n",
    "        # check that the answer is in the subgraph\n",
    "        answer_idx = check_answer_in_subgraph(answer, entity_ids)\n",
    "        if answer_idx:\n",
    "            counts[i] += 1\n",
    "        n_entities.append(len(entity_ids))\n",
    "        \n",
    "print(counts)\n",
    "# show distribution stats\n",
    "print(\"Min: %d Mean: %.2f Max: %d\"%(min(n_entities), np.mean(n_entities), max(n_entities)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
